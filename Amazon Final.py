# ==============================================================================
#
#         OpenAI to Z Challenge: Amazon Archaeological Discovery System
#                           (Unified Comprehensive Edition)
#
#       A Multi-Modal, AI-Powered, Agentic Pipeline for
#                   Archaeological Site Discovery
#
# ==============================================================================
#
# Overview:
# This script represents the definitive, integrated framework for the OpenAI to Z
# Challenge. It combines advanced satellite data processing, AI-powered analysis,
# agentic workflows, and comprehensive data visualization to discover previously
# unknown archaeological sites in the Amazon basin.
#
# Key Features:
# - Multi-source data acquisition (Sentinel-1/2, Landsat, LiDAR, DEM)
# - Advanced anomaly detection algorithms
# - AI-powered site evaluation using OpenAI models
# - Agentic research workflow with iterative refinement
# - Comprehensive HTML reporting with interactive visualizations
# - Vector database integration for historical context
# - Web search capabilities for cross-validation
#
# ==============================================================================

# --- Phase 0: Comprehensive Setup and Initialization ---

import os
import subprocess
import sys
import json
import logging
import time
import base64
import io
import warnings
import hashlib
import httpx
import re
import zipfile
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any, Union, TypedDict, Annotated, Type, TypeVar
from contextlib import redirect_stdout
from dateutil import parser as date_parser

# Scientific Computing and Data Analysis
import numpy as np
import pandas as pd
import geopandas as gpd
from scipy import ndimage
from scipy.stats import zscore
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import DBSCAN, KMeans
try:
    import hdbscan  # type: ignore
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False

# Geospatial and Remote Sensing
import rasterio
import rasterio.plot
from rasterio.plot import show as rio_show
from rasterio.mask import mask as rio_mask
from rasterio.warp import reproject, Resampling, calculate_default_transform
import rioxarray
from shapely.geometry import box, Point, mapping, MultiPoint, Polygon
from shapely.ops import unary_union, transform
from pyproj import Transformer as PyProjTransformer
import contextily as cx

# Visualization
import matplotlib.pyplot as plt
from matplotlib.colors import LightSource
import matplotlib.patches as patches
import seaborn as sns
import folium
from folium import plugins
from PIL import Image

# Google Earth Engine and Remote Sensing APIs
import ee
import geemap
import requests

# AI and Machine Learning
import openai
from openai import OpenAI

# Agentic Workflow Components
from tavily import TavilyClient
from qdrant_client import QdrantClient, models
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage

# Data Validation and Structure
from pydantic import BaseModel, Field

# Biodiversity Data
from pygbif import species as gbif_species, occurrences as gbif_occ

# STAC and Satellite Data
from pystac_client import Client as PystacClient

# Progress Tracking
from tqdm import tqdm

# Environment and Configuration
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)
logging.getLogger("fiona").setLevel(logging.ERROR)
logging.getLogger("rasterio").setLevel(logging.ERROR)

# --- Global Configuration ---
SEED = 42
np.random.seed(SEED)

# Area of Interest - Amazon Basin Focus
AOI_BBOX = {"west": -70.0, "south": -10.5, "east": -68.0, "north": -9.0}
AOI_CRS = "EPSG:4326"
METRIC_CRS = "EPSG:32719"

# AI Model Configuration
LLM_MODEL_VISION = "gpt-4.1-2025-04-14"
LLM_MODEL_ANALYSIS = "gpt-4.1-2025-04-14"
OPENAI_MODEL_O3 = "gpt-4.1-2025-04-14"
OPENAI_MODEL_GPT4 = "gpt-4.1-2025-04-14"

# API Keys and Configuration
API_KEYS = {
    'LLM_API_KEY': "",
    'OPENTOPOGRAPHY_API_KEY': "",
    'TAVILY_API_KEY': "tvly-dev-",
    'QDRANT_URL': "",
    'QDRANT_API_KEY': "",
    'GOOGLE_EARTH_ENGINE_API_KEY': "",
    'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY', '')
}

# Set environment variables
for key, value in API_KEYS.items():
    os.environ[key] = value

# Directory Structure
RUN_TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
BASE_DIR = Path(f"./amazon_z_discovery_{RUN_TIMESTAMP}")
DATA_DIR = BASE_DIR / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
OUTPUT_DIR = BASE_DIR / "outputs"
REPORTS_DIR = BASE_DIR / "reports"
MAPS_DIR = BASE_DIR / "maps"
LOG_DIR = BASE_DIR / "logs"

# Create directory structure
for directory in [BASE_DIR, DATA_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR, 
                  OUTPUT_DIR, REPORTS_DIR, MAPS_DIR, LOG_DIR]:
    directory.mkdir(parents=True, exist_ok=True)

# Logging Configuration
LOG_FILE = LOG_DIR / f"amazon_discovery_{RUN_TIMESTAMP}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

# Create specialized loggers for different components
main_logger = logging.getLogger('MainSystem')
data_logger = logging.getLogger('DataAcquisition')
analysis_logger = logging.getLogger('Analysis')
ai_logger = logging.getLogger('AIAgents')
viz_logger = logging.getLogger('Visualization')

# Set matplotlib style
plt.style.use('seaborn-v0_8-whitegrid')

main_logger.info("="*80)
main_logger.info("Amazon Archaeological Discovery System - OpenAI to Z Challenge")
main_logger.info("="*80)
main_logger.info(f"Run Timestamp: {RUN_TIMESTAMP}")
main_logger.info(f"Base Directory: {BASE_DIR}")
main_logger.info(f"Area of Interest: {AOI_BBOX}")
main_logger.info("="*80)

# ==============================================================================
# --- Phase 1: Data Models and Configuration Classes ---
# ==============================================================================

# --- Pydantic Models for Structured Data ---
class CandidateSite(BaseModel):
    """Model for candidate archaeological sites."""
    coordinates: Tuple[float, float] = Field(..., description="Longitude, latitude coordinates")
    hypothesis: str = Field(..., description="Initial hypothesis for site significance")
    confidence_score: Optional[float] = Field(None, description="Initial confidence score (0-100)")
    site_id: Optional[str] = Field(None, description="Unique site identifier")

class CandidateSites(BaseModel):
    """Collection of candidate sites."""
    sites: List[CandidateSite] = Field(..., description="List of candidate sites")

class RefinedHypothesis(BaseModel):
    """Model for refined research hypothesis."""
    new_hypothesis: str = Field(..., description="Refined hypothesis based on analysis")
    analysis: Optional[str] = Field(None, description="Analysis supporting the hypothesis")
    confidence_change: Optional[float] = Field(None, description="Change in confidence level")

class AnalysisResult(BaseModel):
    """Model for site analysis results."""
    features_found: bool = Field(..., description="Whether archaeological features were identified")
    confidence_score: int = Field(..., description="AI confidence score (0-100)")
    description: str = Field(..., description="Detailed analysis description")
    evidence_summary: Optional[str] = Field(None, description="Summary of supporting evidence")
    risk_factors: Optional[List[str]] = Field(None, description="Potential risk factors or limitations")

class SiteScores(BaseModel):
    """Model for quantitative site scoring."""
    geometric_score: float = Field(..., description="Shape regularity score (0-1)")
    terrain_score: float = Field(..., description="Terrain anomaly significance (0-1)")
    vegetation_score: float = Field(..., description="Vegetation anomaly significance (0-1)")
    priority_score: float = Field(..., description="Combined priority score (0-1)")
    historical_context_score: Optional[float] = Field(None, description="Historical context relevance (0-1)")

# --- Configuration and Client Management ---
class ConfigurationManager:
    """Centralized configuration and client management."""
    
    def __init__(self):
        self.api_keys = API_KEYS
        self.project_id = os.getenv('GOOGLE_EARTH_ENGINE_API_KEY', '')
        self.validate_configuration()
        self.initialize_clients()
        
    def validate_configuration(self):
        """Validate API keys and configuration."""
        required_keys = ['LLM_API_KEY', 'OPENTOPOGRAPHY_API_KEY', 'TAVILY_API_KEY']
        missing_keys = [key for key in required_keys if not self.api_keys.get(key)]
        
        if missing_keys:
            main_logger.warning(f"Missing API keys: {missing_keys}")
        else:
            main_logger.info("All required API keys are configured")
            
    def initialize_clients(self):
        """Initialize all API clients."""
        try:
            # LLM Client for advanced AI analysis
            self.llm_client = OpenAI(
                base_url="https://api.openai.com/v1",
                api_key=self.api_keys['LLM_API_KEY']
            )
            main_logger.info("LLM client initialized")
            
            # OpenAI Client for o3/o4 mini and GPT-4.1
            if self.api_keys.get('OPENAI_API_KEY'):
                self.openai_client = OpenAI(
                    api_key=self.api_keys['OPENAI_API_KEY']
                )
                main_logger.info("OpenAI client initialized")
            else:
                self.openai_client = None
                main_logger.warning("OpenAI API key not found, using LLM only")
            
            # Tavily Client for web search
            self.tavily_client = TavilyClient(self.api_keys['TAVILY_API_KEY'])
            main_logger.info("Tavily client initialized")
            
            # Qdrant Client for vector database
            if self.api_keys.get('QDRANT_URL') and self.api_keys.get('QDRANT_API_KEY'):
                self.qdrant_client = QdrantClient(
                    url=self.api_keys['QDRANT_URL'],
                    api_key=self.api_keys['QDRANT_API_KEY']
                )
                self.setup_qdrant_collection()
                main_logger.info("Qdrant client initialized")
            else:
                self.qdrant_client = None
                main_logger.warning("Qdrant configuration incomplete")
            
            # STAC Client for satellite data
            self.stac_client = PystacClient.open("https://earth-search.aws.element84.com/v1")
            main_logger.info("STAC client initialized")
            
            # Initialize Google Earth Engine
            self.initialize_gee()
            
        except Exception as e:
            main_logger.error(f"Failed to initialize clients: {e}")
            raise
    
    def initialize_gee(self):
        """Initialize Google Earth Engine."""
        try:
            ee.Initialize(project=self.project_id, opt_url='https://earthengine-highvolume.googleapis.com')
            main_logger.info("Google Earth Engine initialized successfully")
        except Exception as e:
            main_logger.error(f"Google Earth Engine initialization failed: {e}")
            main_logger.error("Please authenticate by running 'earthengine authenticate'")
            raise
    
    def setup_qdrant_collection(self, collection_name: str = "amazon_archaeology"):
        """Setup Qdrant collection for archaeological data."""
        if not self.qdrant_client:
            return
            
        try:
            self.qdrant_client.get_collection(collection_name=collection_name)
            main_logger.info(f"Connected to existing Qdrant collection: '{collection_name}'")
        except Exception:
            main_logger.info(f"Creating new Qdrant collection: '{collection_name}'")
            self.qdrant_client.recreate_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(size=384, distance=Distance.COSINE),
            )
            main_logger.info(f"Qdrant collection '{collection_name}' created")
        
        self.qdrant_collection = collection_name

# --- Utility Functions ---
def get_text_embedding(text: str) -> List[float]:
    """Generate text embedding using hash-based approach for Qdrant compatibility."""
    hash_obj = hashlib.sha256(text.encode())
    hash_bytes = hash_obj.digest()
    embedding = []
    
    for i in range(0, min(len(hash_bytes), 48), 1):
        byte_val = hash_bytes[i]
        for bit in range(8):
            embedding.append(float((byte_val >> bit) & 1) * 2.0 - 1.0)
    
    # Pad to 384 dimensions
    while len(embedding) < 384:
        embedding.append(0.0)
    
    return embedding[:384]

def create_site_id(lat: float, lon: float, prefix: str = "AMZN") -> str:
    """Create unique site identifier from coordinates."""
    lat_str = f"{abs(lat):.3f}{'S' if lat < 0 else 'N'}"
    lon_str = f"{abs(lon):.3f}{'W' if lon < 0 else 'E'}"
    return f"{prefix}_{lat_str}_{lon_str}"

def validate_coordinates(lat: float, lon: float) -> bool:
    """Validate that coordinates are within the Amazon basin."""
    amazon_bounds = {
        'min_lat': -18.0, 'max_lat': 6.0,
        'min_lon': -82.0, 'max_lon': -44.0
    }
    
    return (amazon_bounds['min_lat'] <= lat <= amazon_bounds['max_lat'] and
            amazon_bounds['min_lon'] <= lon <= amazon_bounds['max_lon'])

main_logger.info("Configuration and data models initialized successfully")

# ==============================================================================
# --- Phase 2: Comprehensive Data Acquisition System ---
# ==============================================================================

class UnifiedDataAcquisition:
    """Comprehensive data acquisition system combining all data sources."""
    
    def __init__(self, config: ConfigurationManager):
        self.config = config
        self.data_sources = {
            'satellite': ['Landsat 8/9', 'Sentinel-2', 'Sentinel-1 SAR'],
            'elevation': ['SRTM', 'NASA DEM', 'ALOS', 'Copernicus DEM'],
            'biodiversity': ['GBIF occurrences'],
            'historical': ['Archaeological context', 'Colonial records'],
            'lidar': ['OpenTopography'],
            'web': ['Academic papers', 'News articles', 'Research databases']
        }
        data_logger.info("Unified Data Acquisition System initialized")
    
    def create_site_structure(self, site: CandidateSite) -> Tuple[Dict[str, Path], Path]:
        """Create organized folder structure for a site."""
        lon, lat = site.coordinates
        site_id = site.site_id or create_site_id(lat, lon)
        
        site_base = DATA_DIR / site_id
        folders = {
            'landsat': site_base / 'satellite' / 'landsat',
            'sentinel2': site_base / 'satellite' / 'sentinel2',
            'sentinel1': site_base / 'satellite' / 'sentinel1_sar',
            'elevation': site_base / 'elevation',
            'lidar': site_base / 'lidar',
            'biodiversity': site_base / 'biodiversity',
            'historical': site_base / 'historical',
            'processed': site_base / 'processed',
            'metadata': site_base / 'metadata'
        }
        
        for folder_path in folders.values():
            folder_path.mkdir(parents=True, exist_ok=True)
        
        return folders, site_base
    
    def get_sentinel2_data(self, coordinates: Tuple[float, float], folders: Dict[str, Path], 
                          buffer_km: float = 5.0, date_range: int = 365) -> List[str]:
        """Download Sentinel-2 optical data using both STAC and GEE."""
        lon, lat = coordinates
        data_logger.info(f"Acquiring Sentinel-2 data for {lat:.3f}, {lon:.3f}")
        
        downloaded_files = []
        
        # Method 1: STAC API approach
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=date_range)
            
            # Create bounding box
            buffer_deg = buffer_km / 111.0
            bbox = [lon - buffer_deg, lat - buffer_deg, lon + buffer_deg, lat + buffer_deg]
            
            search = self.config.stac_client.search(
                collections=["sentinel-2-l2a"],
                bbox=bbox,
                datetime=f"{start_date.strftime('%Y-%m-%d')}/{end_date.strftime('%Y-%m-%d')}",
                query={"eo:cloud_cover": {"lt": 20}},
                max_items=10
            )
            
            items = sorted(search.item_collection(), 
                         key=lambda item: item.properties.get("eo:cloud_cover", 101))
            
            if items:
                best_item = items[0]
                scene_dict = best_item.to_dict()
                
                # Download key bands
                bands_to_download = {
                    'B04': 'red.tif',    # Red
                    'B08': 'nir.tif',    # NIR
                    'B03': 'green.tif',  # Green
                    'B02': 'blue.tif'    # Blue
                }
                
                for band_key, filename in bands_to_download.items():
                    if band_key in scene_dict['assets']:
                        asset = scene_dict['assets'][band_key]
                        output_path = folders['sentinel2'] / filename
                        try:
                            self._download_stac_asset(asset, output_path)
                            downloaded_files.append(str(output_path))
                        except Exception as e:
                            data_logger.warning(f"Failed to download {band_key}: {e}")
                
                data_logger.info(f"Downloaded {len(downloaded_files)} Sentinel-2 bands via STAC")
            
        except Exception as e:
            data_logger.warning(f"STAC Sentinel-2 download failed: {e}")
        
        # Method 2: Google Earth Engine approach (fallback/supplement)
        try:
            point = ee.Geometry.Point(lon, lat)  # type: ignore
            aoi = point.buffer(buffer_km * 1000).bounds()
            
            end_date = datetime.now()
            start_date = end_date - timedelta(days=date_range)
            
            sentinel2_collection = (
                ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')  # type: ignore
                .filterBounds(aoi)
                .filterDate(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))
                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))  # type: ignore
                .sort('CLOUDY_PIXEL_PERCENTAGE')
                .first()
            )
            
            if sentinel2_collection.getInfo() is not None:
                gee_filename = folders['sentinel2'] / 'sentinel2_gee_composite.tif'
                geemap.download_ee_image(
                    image=sentinel2_collection.clip(aoi),
                    filename=str(gee_filename),
                    region=aoi,
                    scale=10,
                    crs='EPSG:4326'
                )
                downloaded_files.append(str(gee_filename))
                data_logger.info("Downloaded Sentinel-2 composite via GEE")
                
        except Exception as e:
            data_logger.warning(f"GEE Sentinel-2 download failed: {e}")
        
        return downloaded_files
    
    def get_landsat_data(self, coordinates: Tuple[float, float], folders: Dict[str, Path],
                        buffer_km: float = 5.0, date_range: int = 1095) -> List[str]:
        """Download Landsat 8/9 data for multi-temporal analysis."""
        lon, lat = coordinates
        data_logger.info(f"Acquiring Landsat data for {lat:.3f}, {lon:.3f}")
        
        downloaded_files = []
        
        try:
            point = ee.Geometry.Point(lon, lat)  # type: ignore
            aoi = point.buffer(buffer_km * 1000).bounds()
            
            end_date = datetime.now()
            start_date = end_date - timedelta(days=date_range)
            
            # Merge Landsat 8 and 9 collections
            landsat9 = (
                ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')  # type: ignore
                .filterBounds(aoi)
                .filterDate(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))
                .filter(ee.Filter.lt('CLOUD_COVER', 30))  # type: ignore
            )
            
            landsat8 = (
                ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')  # type: ignore
                .filterBounds(aoi)
                .filterDate(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))
                .filter(ee.Filter.lt('CLOUD_COVER', 30))  # type: ignore
            )
            
            merged_collection = landsat9.merge(landsat8).sort('CLOUD_COVER')
            
            # Get seasonal composites
            seasons = [
                ('dry_season', [5, 6, 7, 8, 9]),    # May-September
                ('wet_season', [11, 12, 1, 2, 3])   # November-March
            ]
            
            for season_name, months in seasons:
                seasonal_image = (
                    merged_collection
                    .filter(ee.Filter.calendarRange(months[0], months[-1], 'month'))  # type: ignore
                    .median()
                )
                
                if seasonal_image.getInfo() is not None:
                    filename = folders['landsat'] / f'landsat_{season_name}.tif'
                    try:
                        geemap.download_ee_image(
                            image=seasonal_image.clip(aoi),
                            filename=str(filename),
                            region=aoi,
                            scale=30,
                            crs='EPSG:4326'
                        )
                        downloaded_files.append(str(filename))
                        data_logger.info(f"Downloaded Landsat {season_name}")
                    except Exception as e:
                        data_logger.warning(f"Failed to download Landsat {season_name}: {e}")
            
        except Exception as e:
            data_logger.error(f"Landsat download failed: {e}")
        
        return downloaded_files
    
    def get_sentinel1_sar(self, coordinates: Tuple[float, float], folders: Dict[str, Path],
                         buffer_km: float = 5.0, date_range: int = 365) -> List[str]:
        """Download Sentinel-1 SAR data for vegetation penetration analysis."""
        lon, lat = coordinates
        data_logger.info(f"Acquiring Sentinel-1 SAR data for {lat:.3f}, {lon:.3f}")
        
        downloaded_files = []
        
        try:
            point = ee.Geometry.Point(lon, lat)  # type: ignore
            aoi = point.buffer(buffer_km * 1000).bounds()
            
            end_date = datetime.now()
            start_date = end_date - timedelta(days=date_range)
            
            sentinel1_collection = (
                ee.ImageCollection('COPERNICUS/S1_GRD')  # type: ignore
                .filterBounds(aoi)
                .filterDate(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))
                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))  # type: ignore
                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))  # type: ignore
                .filter(ee.Filter.eq('instrumentMode', 'IW'))  # type: ignore
                .sort('system:time_start', False)
            )
            
            # Get median composite for different polarizations
            polarizations = ['VV', 'VH']
            for pol in polarizations:
                pol_composite = sentinel1_collection.select(pol).median()
                
                if pol_composite.getInfo() is not None:
                    filename = folders['sentinel1'] / f'sentinel1_{pol}.tif'
                    try:
                        geemap.download_ee_image(
                            image=pol_composite.clip(aoi),
                            filename=str(filename),
                            region=aoi,
                            scale=10,
                            crs='EPSG:4326'
                        )
                        downloaded_files.append(str(filename))
                        data_logger.info(f"Downloaded Sentinel-1 {pol} polarization")
                    except Exception as e:
                        data_logger.warning(f"Failed to download SAR {pol}: {e}")
            
        except Exception as e:
            data_logger.error(f"Sentinel-1 SAR download failed: {e}")
        
        return downloaded_files
    
    def get_elevation_data(self, coordinates: Tuple[float, float], folders: Dict[str, Path],
                          buffer_km: float = 5.0) -> List[str]:
        """Download comprehensive elevation data from multiple sources."""
        lon, lat = coordinates
        data_logger.info(f"Acquiring elevation data for {lat:.3f}, {lon:.3f}")
        
        downloaded_files = []
        
        try:
            point = ee.Geometry.Point(lon, lat)  # type: ignore
            aoi = point.buffer(buffer_km * 1000).bounds()
            
            # Priority order of DEM sources
            dem_sources = [
                ('USGS/SRTMGL1_003', 'elevation', 'srtm_30m.tif'),
                ('NASA/NASADEM_HGT/001', 'elevation', 'nasa_dem_30m.tif'),
                ('COPERNICUS/DEM/GLO30', 'DEM', 'copernicus_30m.tif'),
                ('JAXA/ALOS/AW3D30/V3_2', 'DSM', 'alos_30m.tif')
            ]
            
            for dataset_id, band_name, filename in dem_sources:
                try:
                    dem_image = ee.Image(dataset_id).select(band_name).clip(aoi)  # type: ignore
                    
                    # Check if data exists in the region
                    pixel_count = dem_image.reduceRegion(
                        reducer=ee.Reducer.count(),  # type: ignore
                        geometry=aoi,
                        scale=100,
                        maxPixels=1e6
                    ).getInfo()
                    
                    if pixel_count.get(band_name, 0) > 0:
                        output_path = folders['elevation'] / filename
                        geemap.download_ee_image(
                            image=dem_image,
                            filename=str(output_path),
                            region=aoi,
                            scale=30,
                            crs='EPSG:4326'
                        )
                        downloaded_files.append(str(output_path))
                        data_logger.info(f"Downloaded {filename}")
                    else:
                        data_logger.warning(f"No data available for {dataset_id}")
                        
                except Exception as e:
                    data_logger.warning(f"Failed to download {dataset_id}: {e}")
                    continue
            
            # OpenTopography API for high-resolution data
            if self.config.api_keys.get('OPENTOPOGRAPHY_API_KEY'):
                ot_files = self._download_opentopography_dem(coordinates, folders, buffer_km)
                downloaded_files.extend(ot_files)
            
        except Exception as e:
            data_logger.error(f"Elevation data download failed: {e}")
        
        return downloaded_files
    
    def get_biodiversity_data(self, coordinates: Tuple[float, float], folders: Dict[str, Path],
                             buffer_km: float = 10.0) -> Dict[str, Any]:
        """Get biodiversity data relevant to archaeological site indicators."""
        lon, lat = coordinates
        data_logger.info(f"Acquiring biodiversity data for {lat:.3f}, {lon:.3f}")
        
        biodiversity_data = {
            'palm_occurrences': [],
            'archaeological_indicator_species': [],
            'cultural_plants': []
        }
        
        try:
            # Key species for archaeological site detection
            indicator_species = [
                'Bactris gasipaes',     # Peach palm - major archaeological indicator
                'Inga edulis',          # Ice cream bean
                'Theobroma cacao',      # Cacao
                'Manihot esculenta',    # Cassava
                'Zea mays'              # Maize
            ]
            
            buffer_deg = buffer_km / 111.0
            search_bbox = {
                'west': lon - buffer_deg, 'east': lon + buffer_deg,
                'south': lat - buffer_deg, 'north': lat + buffer_deg
            }
            
            for species_name in indicator_species:
                try:
                    # Get species key from GBIF
                    species_info = gbif_species.name_backbone(species_name)
                    if 'usageKey' not in species_info:
                        continue
                    
                    species_key = species_info['usageKey']
                    
                    # Create search geometry
                    geom_wkt = (f"POLYGON(({search_bbox['west']} {search_bbox['north']}, "
                              f"{search_bbox['east']} {search_bbox['north']}, "
                              f"{search_bbox['east']} {search_bbox['south']}, "
                              f"{search_bbox['west']} {search_bbox['south']}, "
                              f"{search_bbox['west']} {search_bbox['north']}))")
                    
                    # Search for occurrences
                    occurrences = gbif_occ.search(
                        taxonKey=species_key,
                        geometry=geom_wkt,
                        hasCoordinate=True,
                        limit=300
                    )
                    
                    species_records = []
                    for record in occurrences.get('results', []):
                        if 'decimalLatitude' in record and 'decimalLongitude' in record:
                            species_records.append({
                                'species': species_name,
                                'lat': record['decimalLatitude'],
                                'lon': record['decimalLongitude'],
                                'year': record.get('year'),
                                'basis': record.get('basisOfRecord')
                            })
                    
                    if species_name == 'Bactris gasipaes':
                        biodiversity_data['palm_occurrences'] = species_records
                    else:
                        biodiversity_data['archaeological_indicator_species'].extend(species_records)
                    
                    data_logger.info(f"Found {len(species_records)} {species_name} occurrences")
                    
                except Exception as e:
                    data_logger.warning(f"Failed to get {species_name} data: {e}")
            
            # Save biodiversity data
            bio_file = folders['biodiversity'] / 'species_occurrences.json'
            with open(bio_file, 'w') as f:
                json.dump(biodiversity_data, f, indent=2)
            
            data_logger.info(f"Biodiversity data saved to {bio_file}")
            
        except Exception as e:
            data_logger.error(f"Biodiversity data acquisition failed: {e}")
        
        return biodiversity_data
    
    def _download_stac_asset(self, asset: Dict[str, Any], output_path: Path):
        """Download STAC asset to file."""
        asset_href = asset.get('href', asset.get('url', ''))
        if not asset_href:
            raise ValueError("No valid href found in asset")
        
        # Open the rasterio dataset first
        xds_raw = rioxarray.open_rasterio(asset_href, masked=True)
        
        # Handle the case where rioxarray returns a list of datasets
        if isinstance(xds_raw, list):
            xds = xds_raw[0]  # Take the first dataset
        else:
            xds = xds_raw
            
        # Now use the properly handled dataset
        xds.rio.to_raster(output_path, compress='LZW', dtype='uint16')
    
    def _download_opentopography_dem(self, coordinates: Tuple[float, float], 
                                   folders: Dict[str, Path], buffer_km: float) -> List[str]:
        """Download high-resolution DEM from OpenTopography."""
        lon, lat = coordinates
        downloaded_files = []
        
        if not self.config.api_keys.get('OPENTOPOGRAPHY_API_KEY'):
            return downloaded_files
        
        try:
            buffer_deg = buffer_km / 111.0
            params = {
                'demtype': 'NASADEM',
                'south': lat - buffer_deg,
                'north': lat + buffer_deg,
                'west': lon - buffer_deg,
                'east': lon + buffer_deg,
                'outputFormat': 'GTiff',
                'API_Key': self.config.api_keys['OPENTOPOGRAPHY_API_KEY']
            }
            
            response = requests.get(
                "https://portal.opentopography.org/API/globaldem",
                params=params,
                timeout=300
            )
            
            if response.status_code == 200:
                filename = folders['elevation'] / 'opentopography_dem.tif'
                with open(filename, 'wb') as f:
                    f.write(response.content)
                downloaded_files.append(str(filename))
                data_logger.info("Downloaded OpenTopography DEM")
            else:
                data_logger.warning(f"OpenTopography request failed: {response.status_code}")
                
        except Exception as e:
            data_logger.warning(f"OpenTopography download failed: {e}")
        
        return downloaded_files
    
    def collect_all_data(self, site: CandidateSite) -> Dict[str, Any]:
        """Collect all available data for a site."""
        data_logger.info(f"Starting comprehensive data collection for site: {site.site_id}")
        
        folders, site_base = self.create_site_structure(site)
        
        # Collect all data types
        collection_results = {
            'site_info': site.model_dump(),
            'folders': {k: str(v) for k, v in folders.items()},
            'base_path': str(site_base),
            'data_files': {},
            'metadata': {
                'collection_timestamp': datetime.now().isoformat(),
                'coordinates': site.coordinates,
                'site_id': site.site_id
            }
        }
        
        try:
            # Satellite data
            collection_results['data_files']['sentinel2'] = self.get_sentinel2_data(
                site.coordinates, folders)
            collection_results['data_files']['landsat'] = self.get_landsat_data(
                site.coordinates, folders)
            collection_results['data_files']['sentinel1'] = self.get_sentinel1_sar(
                site.coordinates, folders)
            
            # Elevation data
            collection_results['data_files']['elevation'] = self.get_elevation_data(
                site.coordinates, folders)
            
            # Biodiversity data
            collection_results['biodiversity_data'] = self.get_biodiversity_data(
                site.coordinates, folders)
            
            # Save metadata
            metadata_file = folders['metadata'] / 'collection_metadata.json'
            with open(metadata_file, 'w') as f:
                json.dump(collection_results['metadata'], f, indent=2)
            
            data_logger.info(f"Data collection completed for {site.site_id}")
            
        except Exception as e:
            data_logger.error(f"Data collection failed for {site.site_id}: {e}")
            collection_results['error'] = str(e)
        
        return collection_results

main_logger.info("Unified Data Acquisition System ready")

# ==============================================================================
# --- Phase 3: Advanced Data Processing and Analysis ---
# ==============================================================================

class DataProcessor:
    """Advanced data processing for satellite and elevation data."""
    
    def __init__(self):
        analysis_logger.info("Data Processor initialized")
    
    def calculate_ndvi(self, red_path: Path, nir_path: Path, output_path: Path) -> bool:
        """Calculate NDVI from red and NIR bands."""
        try:
            with rasterio.open(red_path) as red_src, rasterio.open(nir_path) as nir_src:
                # Get the profile for output
                profile = red_src.profile.copy()
                profile.update(dtype=rasterio.float32, count=1, compress='lzw', nodata=-9999)
                
                # Read and convert to float
                red = red_src.read(1).astype('float32')
                nir = nir_src.read(1).astype('float32')
                
                # Calculate NDVI with error handling
                with np.errstate(divide='ignore', invalid='ignore'):
                    ndvi = (nir - red) / (nir + red)
                
                # Handle invalid values
                ndvi[np.isinf(ndvi) | np.isnan(ndvi)] = -9999
                
                # Write output
                with rasterio.open(output_path, 'w', **profile) as dst:
                    dst.write(ndvi, 1)
                
                analysis_logger.info(f"NDVI calculated and saved to {output_path}")
                return True
                
        except Exception as e:
            analysis_logger.error(f"Failed to calculate NDVI: {e}")
            return False
    
    def calculate_vegetation_indices(self, sentinel2_folder: Path, output_folder: Path) -> Dict[str, str]:
        """Calculate multiple vegetation indices from Sentinel-2 data."""
        indices_calculated = {}
        
        try:
            # Find band files
            band_files: Dict[str, Optional[Path]] = {
                'red': None, 'nir': None, 'green': None, 'blue': None,
                'red_edge': None, 'swir1': None, 'swir2': None
            }
            
            # Map Sentinel-2 files to bands
            for file_path in sentinel2_folder.glob('*.tif'):
                filename = file_path.name.lower()
                if 'red' in filename or 'b04' in filename:
                    band_files['red'] = file_path
                elif 'nir' in filename or 'b08' in filename:
                    band_files['nir'] = file_path
                elif 'green' in filename or 'b03' in filename:
                    band_files['green'] = file_path
                elif 'blue' in filename or 'b02' in filename:
                    band_files['blue'] = file_path
            
            # Calculate NDVI if we have red and NIR
            if band_files['red'] and band_files['nir']:
                ndvi_path = output_folder / 'ndvi.tif'
                if self.calculate_ndvi(band_files['red'], band_files['nir'], ndvi_path):
                    indices_calculated['ndvi'] = str(ndvi_path)
            
            # Calculate other indices if possible
            if (band_files['green'] and band_files['red'] and band_files['nir'] and 
                band_files['blue']):
                # Calculate Enhanced Vegetation Index (EVI)
                evi_path = output_folder / 'evi.tif'
                if self._calculate_evi(band_files['blue'], band_files['red'], 
                                     band_files['nir'], evi_path):
                    indices_calculated['evi'] = str(evi_path)
            
            analysis_logger.info(f"Calculated {len(indices_calculated)} vegetation indices")
            
        except Exception as e:
            analysis_logger.error(f"Vegetation indices calculation failed: {e}")
        
        return indices_calculated
    
    def _calculate_evi(self, blue_path: Path, red_path: Path, nir_path: Path, output_path: Path) -> bool:
        """Calculate Enhanced Vegetation Index (EVI)."""
        try:
            with rasterio.open(blue_path) as blue_src, \
                 rasterio.open(red_path) as red_src, \
                 rasterio.open(nir_path) as nir_src:
                
                profile = red_src.profile.copy()
                profile.update(dtype=rasterio.float32, count=1, compress='lzw', nodata=-9999)
                
                blue = blue_src.read(1).astype('float32')
                red = red_src.read(1).astype('float32')
                nir = nir_src.read(1).astype('float32')
                
                # EVI formula: 2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))
                with np.errstate(divide='ignore', invalid='ignore'):
                    evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1))
                
                evi[np.isinf(evi) | np.isnan(evi)] = -9999
                
                with rasterio.open(output_path, 'w', **profile) as dst:
                    dst.write(evi, 1)
                
                return True
                
        except Exception as e:
            analysis_logger.warning(f"EVI calculation failed: {e}")
            return False
    
    def generate_hillshade(self, dem_path: Path, output_path: Path, 
                          azimuth: float = 315, altitude: float = 45, 
                          vert_exag: float = 5) -> bool:
        """Generate hillshade from DEM."""
        try:
            with rasterio.open(dem_path) as src:
                profile = src.profile.copy()
                profile.update(dtype=rasterio.uint8, nodata=0, compress='lzw')
                
                dem = src.read(1)
                
                # Create hillshade using matplotlib's LightSource
                ls = LightSource(azdeg=azimuth, altdeg=altitude)
                hillshade = ls.hillshade(dem, vert_exag=vert_exag, 
                                       dx=src.res[0], dy=src.res[1])
                
                # Convert to uint8
                hillshade_uint8 = (hillshade * 255).astype(np.uint8)
                
                with rasterio.open(output_path, 'w', **profile) as dst:
                    dst.write(hillshade_uint8, 1)
                
                analysis_logger.info(f"Hillshade generated: {output_path}")
                return True
                
        except Exception as e:
            analysis_logger.error(f"Hillshade generation failed: {e}")
            return False
    
    def calculate_terrain_derivatives(self, dem_path: Path, output_folder: Path) -> Dict[str, str]:
        """Calculate terrain derivatives (slope, aspect, curvature)."""
        derivatives = {}
        
        try:
            with rasterio.open(dem_path) as src:
                dem = src.read(1).astype('float32')
                profile = src.profile.copy()
                profile.update(dtype=rasterio.float32, nodata=-9999, compress='lzw')
                
                # Calculate gradients
                grad_y, grad_x = np.gradient(dem)
                
                # Slope calculation
                slope = np.sqrt(grad_x**2 + grad_y**2)
                slope_path = output_folder / 'slope.tif'
                with rasterio.open(slope_path, 'w', **profile) as dst:
                    dst.write(slope, 1)
                derivatives['slope'] = str(slope_path)
                
                # Aspect calculation
                aspect = np.arctan2(grad_y, grad_x) * 180 / np.pi
                aspect_path = output_folder / 'aspect.tif'
                with rasterio.open(aspect_path, 'w', **profile) as dst:
                    dst.write(aspect, 1)
                derivatives['aspect'] = str(aspect_path)
                
                # Curvature calculation (simplified)
                grad_xx = np.gradient(grad_x, axis=1)
                grad_yy = np.gradient(grad_y, axis=0)
                curvature = grad_xx + grad_yy
                curvature_path = output_folder / 'curvature.tif'
                with rasterio.open(curvature_path, 'w', **profile) as dst:
                    dst.write(curvature, 1)
                derivatives['curvature'] = str(curvature_path)
                
                analysis_logger.info(f"Calculated {len(derivatives)} terrain derivatives")
                
        except Exception as e:
            analysis_logger.error(f"Terrain derivatives calculation failed: {e}")
        
        return derivatives

class AnomalyDetector:
    """Advanced anomaly detection for archaeological features."""
    
    def __init__(self):
        self.detection_methods = ['terrain', 'vegetation', 'sar', 'multi_temporal']
        analysis_logger.info("Anomaly Detector initialized")
    
    def create_analysis_mask(self, dem_path: Path, biodiversity_data: Dict[str, Any], 
                           buffer_km: float = 20.0) -> Tuple[np.ndarray, dict]:
        """Create comprehensive analysis mask combining elevation and biodiversity constraints."""
        with rasterio.open(dem_path) as src:
            dem = src.read(1)
            profile = src.profile.copy()
            
            # Elevation constraints (archaeological sites typically 100-600m in Amazon)
            elev_mask = (dem > 100) & (dem < 600) & (dem != src.nodata)
            
            # Create biodiversity-informed mask
            bio_mask = np.ones_like(dem, dtype=bool)
            
            if biodiversity_data.get('palm_occurrences'):
                palm_points = []
                for occurrence in biodiversity_data['palm_occurrences']:
                    palm_points.append(Point(occurrence['lon'], occurrence['lat']))
                
                if palm_points:
                    # Create buffer around palm occurrences
                    from shapely.geometry import MultiPoint
                    palm_multipoint = MultiPoint(palm_points)
                    
                    # Convert to raster CRS and create buffer
                    import pyproj
                    transformer = pyproj.Transformer.from_crs("EPSG:4326", src.crs, always_xy=True)
                    
                    # Transform points to raster CRS
                    transformed_points = []
                    for point in palm_points:
                        x, y = transformer.transform(point.x, point.y)
                        transformed_points.append(Point(x, y))
                    
                    if transformed_points:
                        buffered_geom = MultiPoint(transformed_points).buffer(buffer_km * 1000)
                        
                        # Create mask from geometry
                        try:
                            palm_mask, _ = rio_mask(src, [mapping(buffered_geom)], invert=False)
                            bio_mask = palm_mask[0].astype(bool)
                        except:
                            analysis_logger.warning("Failed to create biodiversity mask, using full area")
            
            # Combine masks
            final_mask = elev_mask & bio_mask
            
            analysis_logger.info(f"Analysis mask created: {np.sum(final_mask)/final_mask.size*100:.1f}% of area suitable")
            
            return final_mask, profile
    
    def detect_terrain_anomalies(self, dem_path: Path, analysis_mask: np.ndarray, 
                                profile: dict, sensitivity: float = 99.7) -> Optional[gpd.GeoDataFrame]:
        """Detect terrain anomalies using advanced statistical methods."""
        analysis_logger.info("Detecting terrain anomalies...")
        
        try:
            with rasterio.open(dem_path) as src:
                dem = src.read(1).astype('float32')
                
                # Apply mask
                masked_dem = np.where(analysis_mask, dem, np.nan)
                
                # Multi-scale terrain analysis
                scales = [5, 10, 20]  # Different smoothing scales
                anomaly_maps = []
                
                for scale in scales:
                    # Gaussian smoothing at current scale
                    smoothed = ndimage.gaussian_filter(masked_dem, sigma=scale, mode='mirror')
                    smoothed = np.nan_to_num(smoothed, nan=float(np.nanmean(smoothed)))
                    
                    # Calculate residuals
                    residual = masked_dem - smoothed
                    residual[np.isnan(masked_dem)] = 0
                    
                    # Statistical anomaly detection
                    threshold = np.nanpercentile(residual[analysis_mask], sensitivity)
                    anomaly_map = (residual > threshold) & analysis_mask
                    anomaly_maps.append(anomaly_map)
                
                # Combine multi-scale results
                combined_anomalies = np.logical_or.reduce(anomaly_maps)
                
                # Find anomalous pixels
                anomaly_pixels = np.where(combined_anomalies)
                
                if len(anomaly_pixels[0]) > 0:
                    return self._cluster_anomalies(anomaly_pixels, profile, 
                                                 min_cluster_size=30, eps=15)
                else:
                    analysis_logger.warning("No terrain anomalies detected")
                    return None
                    
        except Exception as e:
            analysis_logger.error(f"Terrain anomaly detection failed: {e}")
            return None
    
    def detect_vegetation_anomalies(self, ndvi_path: Path, analysis_mask: np.ndarray, 
                                  profile: dict, sensitivity: float = 99.5) -> Optional[gpd.GeoDataFrame]:
        """Detect vegetation anomalies that may indicate archaeological features."""
        analysis_logger.info("Detecting vegetation anomalies...")
        
        try:
            with rasterio.open(ndvi_path) as src:
                ndvi = src.read(1).astype('float32')
                ndvi_profile = src.profile
                
                # Resample analysis mask if needed
                if analysis_mask.shape != ndvi.shape:
                    from scipy.ndimage import zoom
                    zoom_factors = (ndvi.shape[0] / analysis_mask.shape[0], 
                                  ndvi.shape[1] / analysis_mask.shape[1])
                    resampled_mask = zoom(analysis_mask.astype(float), zoom_factors, order=0) > 0.5
                else:
                    resampled_mask = analysis_mask
                
                # Apply mask
                masked_ndvi = np.where(resampled_mask, ndvi, np.nan)
                
                # Multi-temporal approach (simulate with spatial smoothing)
                temporal_scales = [10, 25, 50]
                anomaly_maps = []
                
                for scale in temporal_scales:
                    smoothed = ndimage.gaussian_filter(masked_ndvi, sigma=scale, mode='mirror')
                    smoothed = np.nan_to_num(smoothed, nan=float(np.nanmean(smoothed)))
                    
                    residual = np.abs(masked_ndvi - smoothed)
                    residual[np.isnan(masked_ndvi)] = 0
                    
                    threshold = np.nanpercentile(residual[resampled_mask], sensitivity)
                    anomaly_map = (residual > threshold) & resampled_mask
                    anomaly_maps.append(anomaly_map)
                
                combined_anomalies = np.logical_or.reduce(anomaly_maps)
                anomaly_pixels = np.where(combined_anomalies)
                
                if len(anomaly_pixels[0]) > 0:
                    return self._cluster_anomalies(anomaly_pixels, ndvi_profile, 
                                                 min_cluster_size=25, eps=12)
                else:
                    analysis_logger.warning("No vegetation anomalies detected")
                    return None
                    
        except Exception as e:
            analysis_logger.error(f"Vegetation anomaly detection failed: {e}")
            return None
    
    def detect_sar_anomalies(self, sar_folder: Path, analysis_mask: np.ndarray) -> Optional[gpd.GeoDataFrame]:
        """Detect SAR anomalies for subsurface features."""
        analysis_logger.info("Detecting SAR anomalies...")
        
        try:
            sar_files = list(sar_folder.glob('*.tif'))
            if not sar_files:
                analysis_logger.warning("No SAR files found")
                return None
            
            # Use the first available SAR file
            sar_path = sar_files[0]
            
            with rasterio.open(sar_path) as src:
                sar_data = src.read(1).astype('float32')
                sar_profile = src.profile
                
                # Resample mask if needed
                if analysis_mask.shape != sar_data.shape:
                    from scipy.ndimage import zoom
                    zoom_factors = (sar_data.shape[0] / analysis_mask.shape[0], 
                                  sar_data.shape[1] / analysis_mask.shape[1])
                    resampled_mask = zoom(analysis_mask.astype(float), zoom_factors, order=0) > 0.5
                else:
                    resampled_mask = analysis_mask
                
                # SAR-specific processing
                masked_sar = np.where(resampled_mask, sar_data, np.nan)
                
                # Edge detection for linear features
                from scipy import ndimage
                sobel_x = ndimage.sobel(masked_sar, axis=0)
                sobel_y = ndimage.sobel(masked_sar, axis=1)
                edge_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
                
                # Detect strong edges that might indicate buried structures
                threshold = np.nanpercentile(edge_magnitude[resampled_mask], 99.0)
                edge_anomalies = (edge_magnitude > threshold) & resampled_mask
                
                anomaly_pixels = np.where(edge_anomalies)
                
                if len(anomaly_pixels[0]) > 0:
                    return self._cluster_anomalies(anomaly_pixels, sar_profile, 
                                                 min_cluster_size=20, eps=10)
                else:
                    analysis_logger.warning("No SAR anomalies detected")
                    return None
                    
        except Exception as e:
            analysis_logger.error(f"SAR anomaly detection failed: {e}")
            return None
    
    def _cluster_anomalies(self, anomaly_pixels: Tuple[Any, ...], 
                          profile: dict, min_cluster_size: int = 25, 
                          eps: int = 10) -> Optional[gpd.GeoDataFrame]:
        """Cluster anomalous pixels into potential archaeological features."""
        try:
            if len(anomaly_pixels[0]) < min_cluster_size:
                analysis_logger.warning("Insufficient anomalous pixels for clustering")
                return None
            
            # Prepare coordinates for clustering
            coords = np.column_stack((anomaly_pixels[0], anomaly_pixels[1]))
            
            # Use DBSCAN or HDBSCAN for clustering
            if HDBSCAN_AVAILABLE:
                clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, 
                                          cluster_selection_epsilon=eps)
                cluster_labels = clusterer.fit_predict(coords)
            else:
                clustering = DBSCAN(eps=eps, min_samples=min_cluster_size)
                cluster_labels = clustering.fit_predict(coords)
            
            # Get unique cluster labels (excluding noise label -1)
            unique_labels = set(cluster_labels) - {-1}
            
            if not unique_labels:
                analysis_logger.warning("No valid clusters found")
                return None
            
            # Create geometries for each cluster
            clusters = []
            for label in unique_labels:
                cluster_coords = coords[cluster_labels == label]
                
                # Convert pixel coordinates to geographic coordinates
                geo_points = []
                for row, col in cluster_coords:
                    x, y = profile['transform'] * (col, row)
                    geo_points.append(Point(x, y))
                
                if len(geo_points) >= 3:
                    # Create convex hull for the cluster
                    cluster_geom = MultiPoint(geo_points).convex_hull
                    clusters.append({
                        'geometry': cluster_geom,
                        'pixel_count': len(cluster_coords),
                        'cluster_id': label
                    })
            
            if clusters:
                gdf = gpd.GeoDataFrame(clusters, crs=profile['crs'])
                analysis_logger.info(f"Found {len(clusters)} anomaly clusters")
                return gdf
            else:
                return None
                
        except Exception as e:
            analysis_logger.error(f"Clustering failed: {e}")
            return None

class FeatureScorer:
    """Comprehensive feature scoring for archaeological site evaluation."""
    
    def __init__(self):
        self.scoring_criteria = {
            'geometric': ['regularity', 'size', 'compactness'],
            'terrain': ['elevation_variance', 'slope_consistency', 'drainage'],
            'vegetation': ['ndvi_anomaly', 'temporal_stability'],
            'contextual': ['proximity_to_water', 'accessibility', 'cultural_indicators']
        }
        analysis_logger.info("Feature Scorer initialized")
    
    def score_comprehensive(self, geometry: Any, data_files: Dict[str, List[str]], 
                          biodiversity_data: Dict[str, Any]) -> SiteScores:
        """Comprehensive scoring using all available data."""
        scores = {
            'geometric_score': self._score_geometry(geometry),
            'terrain_score': self._score_terrain(geometry, data_files.get('elevation', [])),
            'vegetation_score': self._score_vegetation(geometry, data_files),
            'historical_context_score': self._score_historical_context(geometry, biodiversity_data)
        }
        
        # Calculate weighted priority score
        weights = {'geometric': 0.3, 'terrain': 0.3, 'vegetation': 0.25, 'historical_context': 0.15}
        priority_score = sum(scores[f"{k}_score"] * v for k, v in weights.items())
        scores['priority_score'] = priority_score
        
        return SiteScores(**scores)
    
    def _score_geometry(self, geometry: Any) -> float:
        """Score geometric regularity and archaeological relevance."""
        try:
            if geometry.area == 0:
                return 0.0
            
            # Multiple geometric metrics
            perimeter = geometry.boundary.length
            area = geometry.area
            
            # Compactness (Polsby-Popper test)
            compactness = (4 * np.pi * area) / (perimeter ** 2)
            
            # Solidity (area vs convex hull area)
            solidity = area / geometry.convex_hull.area
            
            # Rectangularity (how rectangular is the shape)
            bounds = geometry.bounds
            bbox_area = (bounds[2] - bounds[0]) * (bounds[3] - bounds[1])
            rectangularity = area / bbox_area if bbox_area > 0 else 0
            
            # Combine metrics (archaeological features often have moderate regularity)
            geometric_score = (
                solidity * 0.4 +                    # Prefer solid shapes
                (1 - compactness) * 0.3 +           # Prefer non-circular (more artificial)
                rectangularity * 0.3                # Prefer rectangular features
            )
            
            return np.clip(geometric_score, 0, 1)
            
        except Exception as e:
            analysis_logger.warning(f"Geometric scoring failed: {e}")
            return 0.0
    
    def _score_terrain(self, geometry: Any, elevation_files: List[str]) -> float:
        """Score terrain characteristics for archaeological significance."""
        if not elevation_files:
            return 0.0
        
        try:
            # Use the first available DEM file
            dem_path = elevation_files[0]
            
            with rasterio.open(dem_path) as src:
                # Extract elevation data for the geometry
                dem_data, _ = rio_mask(src, [mapping(geometry)], crop=True, pad=True)
                dem_values = dem_data[0]
                
                # Remove nodata values
                valid_values = dem_values[dem_values != src.nodata]
                
                if len(valid_values) < 10:
                    return 0.0
                
                # Calculate terrain metrics
                elevation_std = np.std(valid_values)
                elevation_range = np.ptp(valid_values)
                mean_elevation = np.mean(valid_values)
                
                # Score based on archaeological preferences
                # Moderate elevation variance suggests human modification
                variance_score = np.clip(elevation_std / 10.0, 0, 1)
                
                # Preferred elevation range for Amazon archaeological sites
                elevation_score = 1.0 if 100 <= mean_elevation <= 600 else 0.5
                
                # Moderate relief suggests terracing or platforms
                relief_score = np.clip(elevation_range / 50.0, 0, 1)
                
                terrain_score = (variance_score * 0.4 + elevation_score * 0.3 + relief_score * 0.3)
                
                return np.clip(terrain_score, 0, 1)
                
        except Exception as e:
            analysis_logger.warning(f"Terrain scoring failed: {e}")
            return 0.0
    
    def _score_vegetation(self, geometry: Any, data_files: Dict[str, List[str]]) -> float:
        """Score vegetation anomalies and patterns."""
        sentinel2_files = data_files.get('sentinel2', [])
        if not sentinel2_files:
            return 0.0
        
        try:
            # Look for NDVI or vegetation index files
            ndvi_file = None
            for file_path in sentinel2_files:
                if 'ndvi' in Path(file_path).name.lower():
                    ndvi_file = file_path
                    break
            
            if not ndvi_file:
                # Try to find NIR and Red bands to calculate NDVI on the fly
                # This is a simplified approach
                return 0.5  # Default score when NDVI is unavailable
            
            with rasterio.open(ndvi_file) as src:
                ndvi_data, _ = rio_mask(src, [mapping(geometry)], crop=True, pad=True)
                ndvi_values = ndvi_data[0]
                
                # Remove invalid values
                valid_ndvi = ndvi_values[(ndvi_values > -1) & (ndvi_values < 1)]
                
                if len(valid_ndvi) < 10:
                    return 0.0
                
                mean_ndvi = np.mean(valid_ndvi)
                ndvi_std = np.std(valid_ndvi)
                
                # Archaeological sites often show vegetation stress or different patterns
                # Lower NDVI than surrounding forest (typical 0.8-0.9) indicates disturbance
                stress_score = np.clip(np.abs(mean_ndvi - 0.85) / 0.3, 0, 1)
                
                # Higher variance suggests heterogeneous vegetation (cultural patterns)
                variance_score = np.clip(ndvi_std / 0.2, 0, 1)
                
                vegetation_score = (stress_score * 0.6 + variance_score * 0.4)
                
                return np.clip(vegetation_score, 0, 1)
                
        except Exception as e:
            analysis_logger.warning(f"Vegetation scoring failed: {e}")
            return 0.0
    
    def _score_historical_context(self, geometry: Any, biodiversity_data: Dict[str, Any]) -> float:
        """Score based on historical and cultural context indicators."""
        try:
            context_score = 0.0
            
            # Proximity to palm occurrences (major archaeological indicator)
            palm_occurrences = biodiversity_data.get('palm_occurrences', [])
            if palm_occurrences:
                min_distance = float('inf')
                centroid = geometry.centroid
                
                for occurrence in palm_occurrences:
                    palm_point = Point(occurrence['lon'], occurrence['lat'])
                    distance = centroid.distance(palm_point)
                    min_distance = min(min_distance, distance)
                
                # Score based on proximity (closer is better, within ~0.1 degrees is excellent)
                palm_score = max(0, 1 - (min_distance / 0.1))
                context_score += palm_score * 0.6
            
            # Other cultural indicator species
            indicator_species = biodiversity_data.get('archaeological_indicator_species', [])
            if indicator_species:
                species_diversity = len(set(sp['species'] for sp in indicator_species))
                diversity_score = min(1.0, species_diversity / 3.0)  # Up to 3 species for max score
                context_score += diversity_score * 0.4
            
            return np.clip(context_score, 0, 1)
            
        except Exception as e:
            analysis_logger.warning(f"Historical context scoring failed: {e}")
            return 0.0

main_logger.info("Data Processing and Analysis System ready")

# ==============================================================================
# --- Phase 4: AI-Powered Analysis and Agentic Workflows ---
# ==============================================================================

# --- Agentic Workflow State ---
class ArchaeologicalState(TypedDict):
    """State for the archaeological research workflow."""
    iteration: int
    max_iterations: int
    site: CandidateSite
    data_collection_results: Dict[str, Any]
    analysis_results: Dict[str, Any]
    hypothesis: str
    confidence_score: float
    evidence_summary: List[str]
    critique: str
    web_search_results: Dict[str, Any]
    vector_search_results: List[Dict[str, Any]]
    ai_evaluations: List[AnalysisResult]
    final_recommendation: Optional[str]

class AIManager:
    """Comprehensive AI analysis using multiple model endpoints."""
    
    def __init__(self, config: ConfigurationManager):
        self.config = config
        self.llm_client = config.llm_client
        self.openai_client = config.openai_client
        self.models = {
            'vision': LLM_MODEL_VISION,
            'analysis': LLM_MODEL_ANALYSIS,
            'o3_mini': OPENAI_MODEL_O3,
            'gpt4': OPENAI_MODEL_GPT4
        }
        ai_logger.info("AI Manager initialized with multiple model endpoints")
    
    def make_structured_request(self, prompt: str, response_model: Type[BaseModel], 
                              model_type: str = 'analysis') -> Optional[Any]:
        """Make structured request to appropriate AI model."""
        try:
            if model_type in ['vision', 'analysis'] and self.llm_client:
                response = self.llm_client.chat.completions.create(
                    model=self.models[model_type],
                    messages=[{"role": "user", "content": prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.2,
                    max_tokens=4096,
                )
                response_content = response.choices[0].message.content
                
                if response_content:
                    # Extract JSON from markdown if needed
                    json_match = re.search(r'```json\n({.*?})\n```', response_content, re.S)
                    if json_match:
                        json_str = json_match.group(1)
                    else:
                        json_str = response_content
                    
                    return response_model.model_validate_json(json_str)
            
            elif model_type in ['o3_mini', 'gpt4'] and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model=self.models[model_type],
                    messages=[{"role": "user", "content": prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.3,
                    max_tokens=4096,
                )
                response_content = response.choices[0].message.content
                
                if response_content:
                    return response_model.model_validate_json(response_content)
            
            return None
            
        except Exception as e:
            ai_logger.error(f"Structured AI request failed: {e}")
            return None
    
    def analyze_site_with_imagery(self, site: CandidateSite, 
                                 data_results: Dict[str, Any],
                                 scores: SiteScores) -> Optional[AnalysisResult]:
        """Comprehensive site analysis using imagery and data."""
        try:
            # Find visualization or processed imagery
            visualization_path = None
            
            # Look for visualization files
            folders = data_results.get('folders', {})
            processed_folder = folders.get('processed')
            
            if processed_folder:
                processed_path = Path(processed_folder)
                # Look for hillshade or other processed imagery
                for image_file in processed_path.glob('*.tif'):
                    if 'hillshade' in image_file.name.lower():
                        visualization_path = image_file
                        break
                
                # Also look for PNG visualizations
                for image_file in processed_path.glob('*.png'):
                    visualization_path = image_file
                    break
            
            # Prepare analysis prompt
            prompt = f"""You are a world-class geoarchaeologist specializing in Amazonian prehistory and remote sensing.
            
SITE INFORMATION:
- Site ID: {site.site_id}
- Coordinates: {site.coordinates[1]:.6f}N, {site.coordinates[0]:.6f}W
- Initial Hypothesis: {site.hypothesis}
- Priority Score: {scores.priority_score:.3f}

QUANTITATIVE ANALYSIS SCORES:
- Geometric Regularity: {scores.geometric_score:.3f} (0-1, higher = more regular/artificial)
- Terrain Anomaly: {scores.terrain_score:.3f} (0-1, higher = more significant relief)
- Vegetation Anomaly: {scores.vegetation_score:.3f} (0-1, higher = more unusual patterns)
- Historical Context: {scores.historical_context_score or 0:.3f} (0-1, higher = more cultural indicators)

DATA AVAILABILITY:
- Satellite Data: {len(data_results.get('data_files', {}).get('sentinel2', []))} Sentinel-2 scenes
- SAR Data: {len(data_results.get('data_files', {}).get('sentinel1', []))} Sentinel-1 scenes
- Elevation Models: {len(data_results.get('data_files', {}).get('elevation', []))} DEMs
- Biodiversity Data: {len(data_results.get('biodiversity_data', {}).get('palm_occurrences', []))} palm occurrences

TASK: Based on this comprehensive data, provide your expert archaeological assessment.

Output as JSON with these exact keys:
- "features_found": boolean (true if you identify convincing archaeological features)
- "confidence_score": integer 0-100 (your confidence this is a genuine archaeological site)
- "description": string (detailed professional assessment integrating all evidence)
- "evidence_summary": string (key evidence points supporting your conclusion)
- "risk_factors": array of strings (potential limitations or alternative explanations)
"""

            # Try to include image analysis if available
            if visualization_path and visualization_path.exists():
                prompt += f"\n\nVISUAL EVIDENCE: Processed imagery available at {visualization_path}"
                
                # For now, use text-based analysis (could be enhanced with vision models)
                result = self.make_structured_request(prompt, AnalysisResult, 'analysis')
            else:
                result = self.make_structured_request(prompt, AnalysisResult, 'analysis')
            
            if result:
                ai_logger.info(f"AI analysis completed for {site.site_id}: confidence {result.confidence_score}%")
                return result
            else:
                ai_logger.warning(f"AI analysis failed for {site.site_id}")
                return None
                
        except Exception as e:
            ai_logger.error(f"Site analysis failed for {site.site_id}: {e}")
            return None
    
    def refine_hypothesis(self, current_hypothesis: str, evidence: Dict[str, Any], 
                         iteration: int) -> Optional[RefinedHypothesis]:
        """Refine research hypothesis based on accumulated evidence."""
        prompt = f"""You are an expert archaeological researcher. Based on the evidence below, 
refine the current research hypothesis for this Amazon archaeological site.

CURRENT HYPOTHESIS (Iteration {iteration}):
{current_hypothesis}

ACCUMULATED EVIDENCE:
{json.dumps(evidence, indent=2)}

Provide a refined hypothesis that:
1. Incorporates new evidence
2. Is more specific and testable
3. Addresses any contradictions
4. Suggests next research steps

Output as JSON with keys:
- "new_hypothesis": refined hypothesis string
- "analysis": explanation of changes and reasoning
- "confidence_change": float indicating confidence change (-1 to +1)
"""
        
        return self.make_structured_request(prompt, RefinedHypothesis, 'analysis')
    
    def generate_critique(self, hypothesis: str, analysis: str, 
                         evidence: Dict[str, Any]) -> str:
        """Generate critical evaluation of current analysis."""
        prompt = f"""You are a skeptical senior archaeologist reviewing this site analysis.
Provide a thorough, critical evaluation.

HYPOTHESIS: {hypothesis}
ANALYSIS: {analysis}
EVIDENCE: {json.dumps(evidence, indent=2)}

Be rigorous. Point out:
- Methodological weaknesses
- Alternative explanations
- Missing data
- Logical inconsistencies
- Potential biases

Provide constructive criticism in 2-3 paragraphs."""

        try:
            if self.llm_client:
                response = self.llm_client.chat.completions.create(
                    model=self.models['analysis'],
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=1000,
                )
                return response.choices[0].message.content or "No critique generated."
            else:
                return "AI critique unavailable - no model client."
                
        except Exception as e:
            ai_logger.error(f"Critique generation failed: {e}")
            return f"Critique generation failed: {e}"

class WebSearchManager:
    """Manages web searches for archaeological context."""
    
    def __init__(self, config: ConfigurationManager):
        self.tavily_client = config.tavily_client
        ai_logger.info("Web Search Manager initialized")
    
    def search_archaeological_context(self, site: CandidateSite, 
                                    hypothesis: str) -> Dict[str, Any]:
        """Search for relevant archaeological and historical context."""
        try:
            # Construct search queries
            queries = [
                f"Amazon archaeological sites {site.coordinates[1]:.1f} {site.coordinates[0]:.1f}",
                f"pre-Columbian settlements Amazon basin",
                f"terra preta archaeological sites Brazil",
                f"Amazonia ancient civilizations {hypothesis}",
                f"indigenous settlements Amazon {site.coordinates[1]:.1f}N {site.coordinates[0]:.1f}W"
            ]
            
            all_results = {"searches": [], "summary": ""}
            
            for query in queries:
                try:
                    results = self.tavily_client.search(
                        query=query,
                        search_depth="advanced",
                        max_results=5
                    )
                    all_results["searches"].append({
                        "query": query,
                        "results": results
                    })
                    ai_logger.info(f"Web search completed: {query}")
                    
                except Exception as e:
                    ai_logger.warning(f"Web search failed for '{query}': {e}")
            
            # Summarize findings
            total_results = sum(len(s.get("results", {}).get("results", [])) 
                              for s in all_results["searches"])
            all_results["summary"] = f"Completed {len(queries)} searches, found {total_results} results"
            
            return all_results
            
        except Exception as e:
            ai_logger.error(f"Web search context failed: {e}")
            return {"error": str(e), "searches": [], "summary": "Web search failed"}

class VectorSearchManager:
    """Manages vector database searches for academic context."""
    
    def __init__(self, config: ConfigurationManager):
        self.qdrant_client = config.qdrant_client
        self.collection_name = getattr(config, 'qdrant_collection', 'amazon_archaeology')
        ai_logger.info("Vector Search Manager initialized")
    
    def search_academic_context(self, hypothesis: str, site: CandidateSite,
                               top_k: int = 5) -> List[Dict[str, Any]]:
        """Search vector database for relevant academic context."""
        if not self.qdrant_client:
            return []
        
        try:
            # Create search vector from hypothesis and site info
            search_text = f"{hypothesis} {site.coordinates} Amazon archaeology"
            search_vector = get_text_embedding(search_text)
            
            search_result = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=search_vector,
                limit=top_k,
                with_payload=True
            )
            
            results = []
            for hit in search_result:
                if hit.payload:
                    results.append({
                        "score": hit.score,
                        "content": hit.payload.get("content", "No content"),
                        "source": hit.payload.get("source", "Unknown source"),
                        "metadata": hit.payload.get("metadata", {})
                    })
            
            ai_logger.info(f"Vector search found {len(results)} academic references")
            return results
            
        except Exception as e:
            ai_logger.error(f"Vector search failed: {e}")
            return []

# --- Agentic Workflow Nodes ---
def data_collection_node(state: ArchaeologicalState) -> Dict[str, Any]:
    """Collect comprehensive data for the site."""
    ai_logger.info(f"--- Data Collection Node: {state['site'].site_id} ---")
    
    try:
        # Initialize data acquisition system
        config = ConfigurationManager()
        data_acq = UnifiedDataAcquisition(config)
        
        # Collect all data for the site
        collection_results = data_acq.collect_all_data(state['site'])
        
        ai_logger.info(f"Data collection completed for {state['site'].site_id}")
        return {"data_collection_results": collection_results}
        
    except Exception as e:
        ai_logger.error(f"Data collection failed: {e}")
        return {"data_collection_results": {"error": str(e)}}

def analysis_node(state: ArchaeologicalState) -> Dict[str, Any]:
    """Perform comprehensive analysis of collected data."""
    ai_logger.info(f"--- Analysis Node: {state['site'].site_id} ---")
    
    try:
        site = state['site']
        data_results = state['data_collection_results']
        
        if 'error' in data_results:
            return {"analysis_results": {"error": "Data collection failed"}}
        
        # Initialize analysis components
        processor = DataProcessor()
        detector = AnomalyDetector()
        scorer = FeatureScorer()
        
        analysis_results = {
            "vegetation_indices": {},
            "terrain_derivatives": {},
            "anomalies": {},
            "scores": None,
            "processed_files": []
        }
        
        # Process data if available
        data_files = data_results.get('data_files', {})
        folders = data_results.get('folders', {})
        
        # Calculate vegetation indices
        if data_files.get('sentinel2') and folders.get('processed'):
            sentinel2_folder = Path(folders['sentinel2'])
            processed_folder = Path(folders['processed'])
            indices = processor.calculate_vegetation_indices(sentinel2_folder, processed_folder)
            analysis_results['vegetation_indices'] = indices
        
        # Calculate terrain derivatives
        elevation_files = data_files.get('elevation', [])
        if elevation_files and folders.get('processed'):
            dem_path = Path(elevation_files[0])
            processed_folder = Path(folders['processed'])
            derivatives = processor.calculate_terrain_derivatives(dem_path, processed_folder)
            analysis_results['terrain_derivatives'] = derivatives
            
            # Generate hillshade
            hillshade_path = processed_folder / 'hillshade.tif'
            processor.generate_hillshade(dem_path, hillshade_path)
            analysis_results['processed_files'].append(str(hillshade_path))
        
        # Anomaly detection
        if elevation_files:
            dem_path = Path(elevation_files[0])
            biodiversity_data = data_results.get('biodiversity_data', {})
            
            # Create analysis mask
            mask, profile = detector.create_analysis_mask(dem_path, biodiversity_data)
            
            # Detect terrain anomalies
            terrain_anomalies = detector.detect_terrain_anomalies(dem_path, mask, profile)
            if terrain_anomalies is not None:
                analysis_results['anomalies']['terrain'] = len(terrain_anomalies)
            
            # Detect vegetation anomalies if NDVI available
            ndvi_files = analysis_results['vegetation_indices'].get('ndvi')
            if ndvi_files:
                veg_anomalies = detector.detect_vegetation_anomalies(
                    Path(ndvi_files), mask, profile)
                if veg_anomalies is not None:
                    analysis_results['anomalies']['vegetation'] = len(veg_anomalies)
            
            # Score the site
            if terrain_anomalies is not None and len(terrain_anomalies) > 0:
                # Use the highest priority anomaly for scoring
                best_anomaly = terrain_anomalies.iloc[0]
                scores = scorer.score_comprehensive(
                    best_anomaly.geometry, data_files, biodiversity_data)
                analysis_results['scores'] = scores.model_dump()
        
        ai_logger.info(f"Analysis completed for {state['site'].site_id}")
        return {"analysis_results": analysis_results}
        
    except Exception as e:
        ai_logger.error(f"Analysis failed: {e}")
        return {"analysis_results": {"error": str(e)}}

def web_search_node(state: ArchaeologicalState) -> Dict[str, Any]:
    """Perform web searches for additional context."""
    ai_logger.info(f"--- Web Search Node: {state['site'].site_id} ---")
    
    try:
        config = ConfigurationManager()
        web_search = WebSearchManager(config)
        
        search_results = web_search.search_archaeological_context(
            state['site'], state['hypothesis'])
        
        return {"web_search_results": search_results}
        
    except Exception as e:
        ai_logger.error(f"Web search failed: {e}")
        return {"web_search_results": {"error": str(e)}}

def vector_search_node(state: ArchaeologicalState) -> Dict[str, Any]:
    """Search vector database for academic context."""
    ai_logger.info(f"--- Vector Search Node: {state['site'].site_id} ---")
    
    try:
        config = ConfigurationManager()
        vector_search = VectorSearchManager(config)
        
        search_results = vector_search.search_academic_context(
            state['hypothesis'], state['site'])
        
        return {"vector_search_results": search_results}
        
    except Exception as e:
        ai_logger.error(f"Vector search failed: {e}")
        return {"vector_search_results": []}

def ai_evaluation_node(state: ArchaeologicalState) -> Dict[str, Any]:
    """AI evaluation and hypothesis refinement."""
    ai_logger.info(f"--- AI Evaluation Node: {state['site'].site_id} ---")
    
    try:
        config = ConfigurationManager()
        ai_manager = AIManager(config)
        
        # Get comprehensive evaluation
        scores_data = state['analysis_results'].get('scores')
        if scores_data:
            scores = SiteScores(**scores_data)
            
            ai_evaluation = ai_manager.analyze_site_with_imagery(
                state['site'], state['data_collection_results'], scores)
            
            if ai_evaluation:
                # Refine hypothesis based on all evidence
                evidence = {
                    "analysis_results": state['analysis_results'],
                    "web_search": state.get('web_search_results', {}),
                    "vector_search": state.get('vector_search_results', []),
                    "ai_evaluation": ai_evaluation.model_dump()
                }
                
                refined = ai_manager.refine_hypothesis(
                    state['hypothesis'], evidence, state['iteration'])
                
                new_hypothesis = refined.new_hypothesis if refined else state['hypothesis']
                confidence_change = refined.confidence_change if refined and refined.confidence_change is not None else 0.0
                
                # Generate critique
                critique = ai_manager.generate_critique(
                    new_hypothesis, 
                    ai_evaluation.description,
                    evidence
                )
                
                return {
                    "ai_evaluations": state.get('ai_evaluations', []) + [ai_evaluation],
                    "hypothesis": new_hypothesis,
                    "confidence_score": max(0, min(100, state.get('confidence_score', 50) + confidence_change * 50)),
                    "critique": critique,
                    "iteration": state['iteration'] + 1
                }
        
        return {
            "iteration": state['iteration'] + 1,
            "critique": "Insufficient data for AI evaluation"
        }
        
    except Exception as e:
        ai_logger.error(f"AI evaluation failed: {e}")
        return {
            "iteration": state['iteration'] + 1,
            "critique": f"AI evaluation failed: {e}"
        }

def should_continue_research(state: ArchaeologicalState) -> str:
    """Determine whether to continue iterative research."""
    iteration = state['iteration']
    max_iterations = state['max_iterations']
    confidence = state.get('confidence_score', 0)
    
    ai_logger.info(f"Research decision: iteration {iteration}/{max_iterations}, confidence {confidence:.1f}%")
    
    # Continue if we haven't reached max iterations and confidence is improving
    if iteration < max_iterations:
        # High confidence sites can finish early
        if confidence > 80:
            ai_logger.info("High confidence reached, finishing research")
            return "finalize"
        else:
            ai_logger.info("Continuing research iteration")
            return "continue"
    else:
        ai_logger.info("Max iterations reached, finalizing")
        return "finalize"

main_logger.info("AI Manager and Agentic Workflow System ready") 

# ==============================================================================
# --- Phase 5: Advanced Visualization and HTML Reporting ---
# ==============================================================================

class VisualizationManager:
    """Comprehensive visualization system for archaeological analysis."""
    
    def __init__(self):
        self.style_config = {
            'figure_size': (20, 12),
            'dpi': 300,
            'color_palette': 'viridis',
            'font_size': 12
        }
        viz_logger.info("Visualization Manager initialized")
    
    def create_site_overview_map(self, site: CandidateSite, data_results: Dict[str, Any], 
                                analysis_results: Dict[str, Any]) -> str:
        """Create comprehensive overview map for a site."""
        try:
            fig, axes = plt.subplots(2, 3, figsize=(24, 16))
            fig.suptitle(f'Archaeological Site Analysis: {site.site_id}\n'
                        f'Coordinates: {site.coordinates[1]:.6f}N, {site.coordinates[0]:.6f}W',
                        fontsize=16, fontweight='bold')
            
            # Get data files
            data_files = data_results.get('data_files', {})
            folders = data_results.get('folders', {})
            
            # Plot 1: Satellite RGB
            self._plot_satellite_rgb(axes[0, 0], data_files.get('sentinel2', []), 
                                   "Sentinel-2 RGB Composite")
            
            # Plot 2: Elevation/Hillshade
            self._plot_elevation_analysis(axes[0, 1], data_files.get('elevation', []),
                                        analysis_results.get('processed_files', []),
                                        "Elevation & Hillshade")
            
            # Plot 3: Vegetation Analysis
            self._plot_vegetation_analysis(axes[0, 2], 
                                         analysis_results.get('vegetation_indices', {}),
                                         "Vegetation Indices")
            
            # Plot 4: SAR Analysis
            self._plot_sar_analysis(axes[1, 0], data_files.get('sentinel1', []),
                                  "SAR Analysis")
            
            # Plot 5: Anomaly Detection
            self._plot_anomaly_summary(axes[1, 1], analysis_results.get('anomalies', {}),
                                     "Detected Anomalies")
            
            # Plot 6: Site Context
            self._plot_site_context(axes[1, 2], site, 
                                   data_results.get('biodiversity_data', {}),
                                   "Archaeological Context")
            
            plt.tight_layout()
            
            # Save the overview map
            output_path = MAPS_DIR / f"{site.site_id}_overview.png"
            plt.savefig(output_path, dpi=self.style_config['dpi'], 
                       bbox_inches='tight', facecolor='white')
            plt.close()
            
            viz_logger.info(f"Overview map created: {output_path}")
            return str(output_path)
            
        except Exception as e:
            viz_logger.error(f"Overview map creation failed: {e}")
            return ""
    
    def _plot_satellite_rgb(self, ax, sentinel2_files: List[str], title: str):
        """Plot Sentinel-2 RGB composite."""
        try:
            if not sentinel2_files:
                ax.text(0.5, 0.5, 'No Sentinel-2 Data Available', 
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(title)
                return
            
            # Find the best RGB file
            rgb_file = None
            for file_path in sentinel2_files:
                if 'gee_composite' in Path(file_path).name:
                    rgb_file = file_path
                    break
            
            if not rgb_file:
                rgb_file = sentinel2_files[0]
            
            with rasterio.open(rgb_file) as src:
                # Read RGB bands (assuming composite has RGB in first 3 bands)
                if src.count >= 3:
                    rgb = src.read([1, 2, 3])
                    rgb = np.transpose(rgb, (1, 2, 0))
                    # Normalize to 0-1 range
                    rgb = np.clip(rgb / np.percentile(rgb, 98), 0, 1)
                    ax.imshow(rgb)
                else:
                    # Single band - use colormap
                    band = src.read(1)
                    im = ax.imshow(band, cmap='viridis')
                    plt.colorbar(im, ax=ax, fraction=0.046)
            
            ax.set_title(title, fontweight='bold')
            ax.axis('off')
            
        except Exception as e:
            ax.text(0.5, 0.5, f'RGB Loading Error:\n{str(e)[:50]}...', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(title)
    
    def _plot_elevation_analysis(self, ax, elevation_files: List[str], 
                                processed_files: List[str], title: str):
        """Plot elevation and hillshade analysis."""
        try:
            # Look for hillshade first
            hillshade_file = None
            for proc_file in processed_files:
                if 'hillshade' in Path(proc_file).name:
                    hillshade_file = proc_file
                    break
            
            if hillshade_file and Path(hillshade_file).exists():
                with rasterio.open(hillshade_file) as src:
                    hillshade = src.read(1)
                    ax.imshow(hillshade, cmap='gray')
            elif elevation_files:
                with rasterio.open(elevation_files[0]) as src:
                    elevation = src.read(1)
                    # Mask nodata values
                    if src.nodata is not None:
                        elevation = np.ma.masked_where(elevation == src.nodata, elevation)
                    im = ax.imshow(elevation, cmap='terrain')
                    plt.colorbar(im, ax=ax, fraction=0.046, label='Elevation (m)')
            else:
                ax.text(0.5, 0.5, 'No Elevation Data Available', 
                       ha='center', va='center', transform=ax.transAxes)
            
            ax.set_title(title, fontweight='bold')
            ax.axis('off')
            
        except Exception as e:
            ax.text(0.5, 0.5, f'Elevation Error:\n{str(e)[:50]}...', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(title)
    
    def _plot_vegetation_analysis(self, ax, vegetation_indices: Dict[str, str], title: str):
        """Plot vegetation index analysis."""
        try:
            if vegetation_indices.get('ndvi'):
                ndvi_file = vegetation_indices['ndvi']
                with rasterio.open(ndvi_file) as src:
                    ndvi = src.read(1)
                    # Mask invalid values
                    ndvi_masked = np.ma.masked_where(ndvi <= -1, ndvi)
                    im = ax.imshow(ndvi_masked, cmap='RdYlGn', vmin=0, vmax=1)
                    plt.colorbar(im, ax=ax, fraction=0.046, label='NDVI')
            else:
                ax.text(0.5, 0.5, 'No Vegetation Indices Available', 
                       ha='center', va='center', transform=ax.transAxes)
            
            ax.set_title(title, fontweight='bold')
            ax.axis('off')
            
        except Exception as e:
            ax.text(0.5, 0.5, f'Vegetation Error:\n{str(e)[:50]}...', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(title)
    
    def _plot_sar_analysis(self, ax, sar_files: List[str], title: str):
        """Plot SAR analysis."""
        try:
            if sar_files:
                with rasterio.open(sar_files[0]) as src:
                    sar_data = src.read(1)
                    im = ax.imshow(sar_data, cmap='gray')
                    plt.colorbar(im, ax=ax, fraction=0.046, label='Backscatter')
            else:
                ax.text(0.5, 0.5, 'No SAR Data Available', 
                       ha='center', va='center', transform=ax.transAxes)
            
            ax.set_title(title, fontweight='bold')
            ax.axis('off')
            
        except Exception as e:
            ax.text(0.5, 0.5, f'SAR Error:\n{str(e)[:50]}...', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(title)
    
    def _plot_anomaly_summary(self, ax, anomalies: Dict[str, Any], title: str):
        """Plot anomaly detection summary."""
        try:
            # Create a summary visualization
            anomaly_types = list(anomalies.keys())
            anomaly_counts = [anomalies.get(atype, 0) for atype in anomaly_types]
            
            if anomaly_types:
                colors = plt.cm.get_cmap('Set3')(np.linspace(0, 1, len(anomaly_types)))
                bars = ax.bar(anomaly_types, anomaly_counts, color=colors)
                ax.set_ylabel('Number of Anomalies')
                ax.set_title(title, fontweight='bold')
                
                # Add value labels on bars
                for bar, count in zip(bars, anomaly_counts):
                    if count > 0:
                        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                               str(count), ha='center', va='bottom')
            else:
                ax.text(0.5, 0.5, 'No Anomalies Detected', 
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(title, fontweight='bold')
            
        except Exception as e:
            ax.text(0.5, 0.5, f'Anomaly Error:\n{str(e)[:50]}...', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(title)
    
    def _plot_site_context(self, ax, site: CandidateSite, 
                          biodiversity_data: Dict[str, Any], title: str):
        """Plot archaeological site context."""
        try:
            # Create a context visualization
            context_info = []
            
            # Palm occurrences
            palm_count = len(biodiversity_data.get('palm_occurrences', []))
            if palm_count > 0:
                context_info.append(f"Palm occurrences: {palm_count}")
            
            # Other indicator species
            indicator_count = len(biodiversity_data.get('archaeological_indicator_species', []))
            if indicator_count > 0:
                context_info.append(f"Cultural indicators: {indicator_count}")
            
            # Site hypothesis
            context_info.append(f"Hypothesis: {site.hypothesis[:50]}...")
            
            if context_info:
                ax.text(0.05, 0.95, '\n'.join(context_info), 
                       transform=ax.transAxes, fontsize=10,
                       verticalalignment='top', bbox=dict(boxstyle='round', 
                       facecolor='lightblue', alpha=0.8))
            
            # Plot palm locations if available
            if palm_count > 0:
                palm_lons = [p['lon'] for p in biodiversity_data['palm_occurrences']]
                palm_lats = [p['lat'] for p in biodiversity_data['palm_occurrences']]
                
                # Simple scatter plot (could be enhanced with proper mapping)
                ax.scatter(palm_lons, palm_lats, c='red', s=50, alpha=0.7, label='Palm occurrences')
                ax.scatter([site.coordinates[0]], [site.coordinates[1]], 
                          c='blue', s=100, marker='*', label='Target site')
                ax.legend()
                ax.set_xlabel('Longitude')
                ax.set_ylabel('Latitude')
            else:
                ax.text(0.5, 0.5, 'Limited Context Data Available', 
                       ha='center', va='center', transform=ax.transAxes)
            
            ax.set_title(title, fontweight='bold')
            
        except Exception as e:
            ax.text(0.5, 0.5, f'Context Error:\n{str(e)[:50]}...', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(title)
    
    def create_interactive_map(self, sites: List[CandidateSite], 
                              results: List[Dict[str, Any]]) -> str:
        """Create interactive Folium map for all analyzed sites."""
        try:
            # Calculate center point
            if sites:
                center_lat = float(np.mean([site.coordinates[1] for site in sites]))
                center_lon = float(np.mean([site.coordinates[0] for site in sites]))
            else:
                center_lat, center_lon = -9.5, -69.0  # Default Amazon center
            
            # Create base map
            m = folium.Map(
                location=[center_lat, center_lon],
                zoom_start=8,
                tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',
                attr='Esri World Imagery'
            )
            
            # Add sites to map
            for i, (site, result) in enumerate(zip(sites, results)):
                # Get analysis results
                ai_evaluations = result.get('ai_evaluations', [])
                confidence = result.get('confidence_score', 0)
                
                # Determine marker color based on confidence
                if confidence > 75:
                    marker_color = 'red'
                    icon = 'star'
                elif confidence > 50:
                    marker_color = 'orange'
                    icon = 'info-sign'
                else:
                    marker_color = 'blue'
                    icon = 'question-sign'
                
                # Create popup content
                popup_html = self._create_site_popup(site, result, i+1)
                
                # Add marker
                folium.Marker(
                    location=[site.coordinates[1], site.coordinates[0]],
                    popup=folium.Popup(popup_html, max_width=400),
                    tooltip=f"Site {i+1}: {site.site_id}",
                    icon=folium.Icon(color=marker_color, icon=icon)
                ).add_to(m)
            
            # Add legend
            legend_html = self._create_map_legend()
            # m.get_root().html.add_child(folium.Element(legend_html))  # Commented out due to compatibility issues
            
            # Save map
            map_path = MAPS_DIR / "interactive_sites_map.html"
            m.save(str(map_path))
            
            viz_logger.info(f"Interactive map created: {map_path}")
            return str(map_path)
            
        except Exception as e:
            viz_logger.error(f"Interactive map creation failed: {e}")
            return ""
    
    def _create_site_popup(self, site: CandidateSite, result: Dict[str, Any], 
                          site_number: int) -> str:
        """Create HTML popup content for site marker."""
        ai_evaluations = result.get('ai_evaluations', [])
        confidence = result.get('confidence_score', 0)
        
        # Get latest AI evaluation
        latest_eval = ai_evaluations[-1] if ai_evaluations else None
        
        html = f"""
        <div style="width: 350px; font-family: Arial, sans-serif;">
            <h4><b>Site {site_number}: {site.site_id}</b></h4>
            <hr>
            <p><b>Coordinates:</b> {site.coordinates[1]:.6f}N, {site.coordinates[0]:.6f}W</p>
            <p><b>Confidence:</b> {confidence:.1f}%</p>
            <p><b>Hypothesis:</b> {site.hypothesis}</p>
            
            {f'<p><b>AI Assessment:</b> {latest_eval.description[:200]}...</p>' if latest_eval else ''}
            
            <details>
                <summary><b>Technical Details</b></summary>
                <p><b>Iterations:</b> {result.get('iteration', 0)}</p>
                <p><b>Data Sources:</b> 
                    Sentinel-2: {len(result.get('data_collection_results', {}).get('data_files', {}).get('sentinel2', []))}, 
                    SAR: {len(result.get('data_collection_results', {}).get('data_files', {}).get('sentinel1', []))}, 
                    DEMs: {len(result.get('data_collection_results', {}).get('data_files', {}).get('elevation', []))}
                </p>
                {f'<p><b>Anomalies:</b> {result.get("analysis_results", {}).get("anomalies", {})}</p>' if result.get('analysis_results', {}).get('anomalies') else ''}
            </details>
        </div>
        """
        return html
    
    def _create_map_legend(self) -> str:
        """Create HTML legend for the map."""
        return """
        <div style='position: fixed; 
                    top: 10px; right: 10px; width: 200px; height: 120px; 
                    background-color: white; border:2px solid grey; z-index:9999; 
                    font-size:14px; padding: 10px'>
        <h4>Site Confidence</h4>
        <p><i class="fa fa-star" style="color:red"></i> High (>75%)</p>
        <p><i class="fa fa-info-circle" style="color:orange"></i> Medium (50-75%)</p>
        <p><i class="fa fa-question-circle" style="color:blue"></i> Low (<50%)</p>
        </div>
        """

class HTMLReportGenerator:
    """Generate comprehensive HTML reports for the OpenAI to Z Challenge."""
    
    def __init__(self):
        self.template_style = """
        <style>
            body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 40px; background-color: #f5f5f5; }
            .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; }
            .container { background: white; padding: 30px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin-bottom: 20px; }
            .site-card { border: 2px solid #ddd; border-radius: 10px; padding: 20px; margin: 20px 0; background: #fafafa; }
            .high-confidence { border-color: #28a745; background: #f8fff9; }
            .medium-confidence { border-color: #ffc107; background: #fffef8; }
            .low-confidence { border-color: #dc3545; background: #fff8f8; }
            .score-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }
            .score-item { background: #e9ecef; padding: 15px; border-radius: 8px; text-align: center; }
            .evidence-list { background: #f8f9fa; padding: 15px; border-left: 4px solid #007bff; margin: 10px 0; }
            .methodology { background: #fff3cd; padding: 20px; border-radius: 10px; border: 1px solid #ffeaa7; }
            .data-source { display: inline-block; background: #6c757d; color: white; padding: 5px 10px; margin: 2px; border-radius: 15px; font-size: 12px; }
            .image-gallery { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
            .image-card { border: 1px solid #ddd; border-radius: 8px; overflow: hidden; }
            .image-card img { width: 100%; height: 200px; object-fit: cover; }
            .image-caption { padding: 10px; background: #f8f9fa; font-size: 14px; }
            h1, h2, h3 { color: #2c3e50; }
            .confidence-bar { width: 100%; background: #e0e0e0; border-radius: 10px; overflow: hidden; height: 20px; }
            .confidence-fill { height: 100%; background: linear-gradient(90deg, #dc3545 0%, #ffc107 50%, #28a745 100%); }
            .coordinates { font-family: monospace; background: #f1f3f4; padding: 5px 10px; border-radius: 5px; }
        </style>
        """
        viz_logger.info("HTML Report Generator initialized")
    
    def generate_comprehensive_report(self, expedition_results: Dict[str, Any]) -> str:
        """Generate the complete HTML report for the OpenAI to Z Challenge."""
        try:
            # Extract key information
            sites_analyzed = expedition_results.get('sites_analyzed', [])
            research_summary = expedition_results.get('research_summary', '')
            recommended_site = expedition_results.get('recommended_site')
            methodology = expedition_results.get('methodology', {})
            
            # Start building HTML
            html_content = f"""
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>OpenAI to Z Challenge: Amazon Archaeological Discovery</title>
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
                {self.template_style}
            </head>
            <body>
                <div class="header">
                    <h1><i class="fas fa-map-marked-alt"></i> OpenAI to Z Challenge: Amazon Archaeological Discovery</h1>
                    <p style="font-size: 18px; margin: 10px 0;">
                        Comprehensive AI-Powered Analysis of Previously Unknown Archaeological Sites
                    </p>
                    <p style="opacity: 0.9;">
                        <i class="fas fa-calendar"></i> Analysis Date: {datetime.now().strftime('%B %d, %Y')} | 
                        <i class="fas fa-map-pin"></i> Focus Region: Amazon Basin | 
                        <i class="fas fa-microscope"></i> Sites Analyzed: {len(sites_analyzed)}
                    </p>
                </div>
            """
            
            # Executive Summary
            html_content += self._generate_executive_summary(recommended_site, sites_analyzed)
            
            # Methodology Section
            html_content += self._generate_methodology_section(methodology)
            
            # Recommended Site Section
            if recommended_site:
                html_content += self._generate_recommended_site_section(recommended_site)
            
            # All Sites Analysis
            html_content += self._generate_all_sites_section(sites_analyzed)
            
            # Data Sources and Reproducibility
            html_content += self._generate_reproducibility_section(sites_analyzed)
            
            # Conclusion
            html_content += self._generate_conclusion_section()
            
            html_content += """
            </body>
            </html>
            """
            
            # Save report
            report_path = REPORTS_DIR / f"amazon_discovery_report_{RUN_TIMESTAMP}.html"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            viz_logger.info(f"Comprehensive HTML report generated: {report_path}")
            return str(report_path)
            
        except Exception as e:
            viz_logger.error(f"HTML report generation failed: {e}")
            return ""
    
    def _generate_executive_summary(self, recommended_site: Optional[Dict[str, Any]], 
                                   sites_analyzed: List[Dict[str, Any]]) -> str:
        """Generate executive summary section."""
        if recommended_site:
            site_info = recommended_site.get('site', {})
            confidence = recommended_site.get('confidence_score', 0)
            coordinates = site_info.get('coordinates', [0, 0])
            
            summary = f"""
            <div class="container">
                <h2><i class="fas fa-star"></i> Executive Summary</h2>
                <div class="site-card high-confidence">
                    <h3> RECOMMENDED DISCOVERY: {site_info.get('site_id', 'Unknown')}</h3>
                    <p><strong>Location:</strong> <span class="coordinates">{coordinates[1]:.6f}N, {coordinates[0]:.6f}W</span></p>
                    <p><strong>Confidence Level:</strong></p>
                    <div class="confidence-bar">
                        <div class="confidence-fill" style="width: {confidence}%;"></div>
                    </div>
                    <p style="text-align: center; margin: 10px 0;"><strong>{confidence:.1f}% Confidence</strong></p>
                    <p><strong>Archaeological Significance:</strong> {site_info.get('hypothesis', 'No hypothesis available')}</p>
                </div>
                
                <div class="score-grid">
                    <div class="score-item">
                        <h4>Sites Analyzed</h4>
                        <div style="font-size: 24px; color: #007bff;">{len(sites_analyzed)}</div>
                    </div>
                    <div class="score-item">
                        <h4>High Confidence Sites</h4>
                        <div style="font-size: 24px; color: #28a745;">{len([s for s in sites_analyzed if s.get('confidence_score', 0) > 75])}</div>
                    </div>
                    <div class="score-item">
                        <h4>Data Sources Used</h4>
                        <div style="font-size: 24px; color: #6f42c1;">7+</div>
                        <small>Satellite, LiDAR, SAR, Biodiversity</small>
                    </div>
                    <div class="score-item">
                        <h4>AI Models Deployed</h4>
                        <div style="font-size: 24px; color: #fd7e14;">3</div>
                        <small>GPT-4.1, o3-mini, LLM</small>
                    </div>
                </div>
            </div>
            """
        else:
            summary = f"""
            <div class="container">
                <h2><i class="fas fa-info-circle"></i> Executive Summary</h2>
                <div class="site-card medium-confidence">
                    <h3> ANALYSIS COMPLETED</h3>
                    <p>Comprehensive analysis of {len(sites_analyzed)} potential archaeological sites in the Amazon basin has been completed using advanced AI and remote sensing techniques.</p>
                    <p>While no sites exceeded the high-confidence threshold for definitive recommendation, valuable insights have been gained about the methodology and potential for future discoveries.</p>
                </div>
            </div>
            """
        
        return summary
    
    def _generate_methodology_section(self, methodology: Dict[str, Any]) -> str:
        """Generate methodology section."""
        return f"""
        <div class="container">
            <h2><i class="fas fa-cogs"></i> Methodology</h2>
            <div class="methodology">
                <h3> Multi-Modal AI-Powered Archaeological Detection Pipeline</h3>
                <p>Our approach combines cutting-edge AI models with comprehensive remote sensing data to identify previously unknown archaeological sites in the Amazon basin.</p>
                
                <h4> Data Acquisition</h4>
                <ul>
                    <li><strong>Satellite Imagery:</strong> Sentinel-2 optical (10m resolution), Landsat 8/9 (30m resolution)</li>
                    <li><strong>SAR Data:</strong> Sentinel-1 C-band SAR for vegetation penetration</li>
                    <li><strong>Elevation Models:</strong> SRTM, NASA DEM, ALOS, Copernicus DEM (30m resolution)</li>
                    <li><strong>Biodiversity Data:</strong> GBIF species occurrences for cultural indicator plants</li>
                    <li><strong>LiDAR:</strong> OpenTopography high-resolution elevation data where available</li>
                </ul>
                
                <h4> AI Analysis Pipeline</h4>
                <ol>
                    <li><strong>Anomaly Detection:</strong> Multi-scale statistical analysis for terrain and vegetation anomalies</li>
                    <li><strong>Feature Scoring:</strong> Quantitative assessment of geometric, terrain, and vegetation characteristics</li>
                    <li><strong>AI Evaluation:</strong> Expert-level assessment using GPT-4.1 and o3-mini models</li>
                    <li><strong>Iterative Refinement:</strong> Hypothesis refinement through agentic workflow</li>
                    <li><strong>Cross-Validation:</strong> Web search and vector database validation</li>
                </ol>
                
                <h4> Evaluation Criteria</h4>
                <div class="score-grid">
                    <div class="score-item">
                        <strong>Geometric Regularity</strong><br>
                        <small>Shape analysis for artificial features</small>
                    </div>
                    <div class="score-item">
                        <strong>Terrain Anomalies</strong><br>
                        <small>Elevation variance and modifications</small>
                    </div>
                    <div class="score-item">
                        <strong>Vegetation Patterns</strong><br>
                        <small>NDVI anomalies and disturbances</small>
                    </div>
                    <div class="score-item">
                        <strong>Cultural Context</strong><br>
                        <small>Proximity to indicator species</small>
                    </div>
                </div>
            </div>
        </div>
        """
    
    def _generate_recommended_site_section(self, recommended_site: Dict[str, Any]) -> str:
        """Generate detailed section for recommended site."""
        site_info = recommended_site.get('site', {})
        analysis_results = recommended_site.get('analysis_results', {})
        ai_evaluations = recommended_site.get('ai_evaluations', [])
        
        # Get latest AI evaluation
        latest_eval = ai_evaluations[-1] if ai_evaluations else None
        
        return f"""
        <div class="container">
            <h2><i class="fas fa-trophy"></i> Recommended Discovery: {site_info.get('site_id', 'Unknown')}</h2>
            
            <div class="site-card high-confidence">
                <h3> Site Details</h3>
                <div class="score-grid">
                    <div class="score-item">
                        <strong>Coordinates</strong><br>
                        <span class="coordinates">{site_info.get('coordinates', [0, 0])[1]:.6f}N, {site_info.get('coordinates', [0, 0])[0]:.6f}W</span>
                    </div>
                    <div class="score-item">
                        <strong>Site ID</strong><br>
                        {site_info.get('site_id', 'Unknown')}
                    </div>
                    <div class="score-item">
                        <strong>Confidence Score</strong><br>
                        {recommended_site.get('confidence_score', 0):.1f}%
                    </div>
                    <div class="score-item">
                        <strong>Analysis Iterations</strong><br>
                        {recommended_site.get('iteration', 0)}
                    </div>
                </div>
                
                <h4> Archaeological Hypothesis</h4>
                <div class="evidence-list">
                    <p>{site_info.get('hypothesis', 'No hypothesis available')}</p>
                </div>
                
                {f'''
                <h4> AI Expert Assessment</h4>
                <div class="evidence-list">
                    <p><strong>Features Identified:</strong> {"Yes" if latest_eval.features_found else "No"}</p>
                    <p><strong>Analysis:</strong> {latest_eval.description}</p>
                    {f"<p><strong>Supporting Evidence:</strong> {latest_eval.evidence_summary}</p>" if latest_eval.evidence_summary else ""}
                    {f"<p><strong>Risk Factors:</strong> {', '.join(latest_eval.risk_factors)}</p>" if latest_eval.risk_factors else ""}
                </div>
                ''' if latest_eval else ''}
                
                <h4> Quantitative Scores</h4>
                {self._generate_scores_display(analysis_results.get('scores', {}))}
                
                <h4> Visualization Maps</h4>
                <p>Comprehensive analysis maps have been generated for this site, including:</p>
                <ul>
                    <li>Multi-spectral satellite imagery analysis</li>
                    <li>Elevation and hillshade modeling</li>
                    <li>Vegetation index anomaly detection</li>
                    <li>SAR penetration analysis</li>
                    <li>Archaeological context mapping</li>
                </ul>
            </div>
        </div>
        """
    
    def _generate_scores_display(self, scores: Dict[str, Any]) -> str:
        """Generate HTML display for quantitative scores."""
        if not scores:
            return "<p>No quantitative scores available.</p>"
        
        return f"""
        <div class="score-grid">
            <div class="score-item">
                <strong>Geometric Score</strong><br>
                <div style="font-size: 20px; color: #007bff;">{scores.get('geometric_score', 0):.3f}</div>
                <small>Shape regularity (0-1)</small>
            </div>
            <div class="score-item">
                <strong>Terrain Score</strong><br>
                <div style="font-size: 20px; color: #28a745;">{scores.get('terrain_score', 0):.3f}</div>
                <small>Elevation anomalies (0-1)</small>
            </div>
            <div class="score-item">
                <strong>Vegetation Score</strong><br>
                <div style="font-size: 20px; color: #ffc107;">{scores.get('vegetation_score', 0):.3f}</div>
                <small>NDVI patterns (0-1)</small>
            </div>
            <div class="score-item">
                <strong>Priority Score</strong><br>
                <div style="font-size: 20px; color: #dc3545;">{scores.get('priority_score', 0):.3f}</div>
                <small>Combined metric (0-1)</small>
            </div>
        </div>
        """
    
    def _generate_all_sites_section(self, sites_analyzed: List[Dict[str, Any]]) -> str:
        """Generate section showing all analyzed sites."""
        sites_html = f"""
        <div class="container">
            <h2><i class="fas fa-list"></i> Complete Site Analysis ({len(sites_analyzed)} sites)</h2>
        """
        
        # Sort sites by confidence score
        sorted_sites = sorted(sites_analyzed, 
                            key=lambda x: x.get('confidence_score', 0), 
                            reverse=True)
        
        for i, site_result in enumerate(sorted_sites):
            site_info = site_result.get('site', {})
            confidence = site_result.get('confidence_score', 0)
            
            # Determine confidence class
            if confidence > 75:
                confidence_class = "high-confidence"
                confidence_icon = ""
            elif confidence > 50:
                confidence_class = "medium-confidence" 
                confidence_icon = ""
            else:
                confidence_class = "low-confidence"
                confidence_icon = ""
            
            sites_html += f"""
            <div class="site-card {confidence_class}">
                <h3>{confidence_icon} Site {i+1}: {site_info.get('site_id', 'Unknown')}</h3>
                <div style="display: grid; grid-template-columns: 2fr 1fr; gap: 20px;">
                    <div>
                        <p><strong>Location:</strong> <span class="coordinates">{site_info.get('coordinates', [0, 0])[1]:.6f}N, {site_info.get('coordinates', [0, 0])[0]:.6f}W</span></p>
                        <p><strong>Hypothesis:</strong> {site_info.get('hypothesis', 'No hypothesis available')}</p>
                        <p><strong>Analysis Iterations:</strong> {site_result.get('iteration', 0)}</p>
                    </div>
                    <div>
                        <div class="confidence-bar">
                            <div class="confidence-fill" style="width: {confidence}%;"></div>
                        </div>
                        <p style="text-align: center; margin: 10px 0;"><strong>{confidence:.1f}%</strong></p>
                    </div>
                </div>
                
                <div class="data-source">Sentinel-2: {len(site_result.get('data_collection_results', {}).get('data_files', {}).get('sentinel2', []))}</div>
                <div class="data-source">SAR: {len(site_result.get('data_collection_results', {}).get('data_files', {}).get('sentinel1', []))}</div>
                <div class="data-source">DEM: {len(site_result.get('data_collection_results', {}).get('data_files', {}).get('elevation', []))}</div>
                <div class="data-source">Biodiversity: {len(site_result.get('data_collection_results', {}).get('biodiversity_data', {}).get('palm_occurrences', []))}</div>
            </div>
            """
        
        sites_html += "</div>"
        return sites_html
    
    def _generate_reproducibility_section(self, sites_analyzed: List[Dict[str, Any]]) -> str:
        """Generate reproducibility and data sources section."""
        return f"""
         <div class="container">
             <h2><i class="fas fa-database"></i> Data Sources & Reproducibility</h2>
             <div class="methodology">
                 <h3> Verifiable Public Data Sources</h3>
                 <ul>
                     <li><strong>Sentinel-2 L2A:</strong> <a href="https://earth-search.aws.element84.com/v1">https://earth-search.aws.element84.com/v1</a></li>
                     <li><strong>Landsat Collection 2:</strong> Google Earth Engine public datasets</li>
                     <li><strong>SRTM DEM:</strong> USGS/SRTMGL1_003 via Google Earth Engine</li>
                     <li><strong>NASA DEM:</strong> NASA/NASADEM_HGT/001 via Google Earth Engine</li>
                     <li><strong>Biodiversity Data:</strong> <a href="https://www.gbif.org/">Global Biodiversity Information Facility (GBIF)</a></li>
                     <li><strong>OpenTopography API:</strong> <a href="https://portal.opentopography.org/API/globaldem">https://portal.opentopography.org/API/globaldem</a></li>
                 </ul>
                 
                 <h3> Reproducibility</h3>
                 <p>This analysis is fully reproducible using the provided methodology and public data sources. All coordinates, algorithms, and data processing steps are documented. The complete analysis pipeline is implemented in Python using open-source libraries.</p>
                 
                 <h4>Key Software Dependencies:</h4>
                 <div class="data-source">Python 3.8+</div>
                 <div class="data-source">Google Earth Engine</div>
                 <div class="data-source">Rasterio</div>
                 <div class="data-source">GeoPandas</div>
                 <div class="data-source">Scikit-learn</div>
                 <div class="data-source">OpenAI API</div>
                 <div class="data-source">Folium</div>
                 <div class="data-source">Matplotlib</div>
                 
                 <h4> Analysis Timeline</h4>
                 <p><strong>Total Runtime:</strong> Approximately 2-4 hours for complete analysis of 10 sites<br>
                 <strong>Data Download:</strong> ~30-60 minutes per site depending on data availability<br>
                 <strong>Processing & AI Analysis:</strong> ~10-15 minutes per site<br>
                 <strong>Report Generation:</strong> ~5 minutes</p>
             </div>
         </div>
         """
    
    def _generate_conclusion_section(self) -> str:
        """Generate conclusion section."""
        return f"""
        <div class="container">
            <h2><i class="fas fa-flag-checkered"></i> Conclusion</h2>
            <div class="methodology">
                <h3> Key Achievements</h3>
                <ul>
                    <li>Successfully deployed a comprehensive AI-powered archaeological detection pipeline</li>
                    <li>Integrated multiple remote sensing data sources with advanced AI analysis</li>
                    <li>Demonstrated reproducible methodology using entirely public data sources</li>
                    <li>Applied agentic workflows for iterative hypothesis refinement</li>
                    <li>Generated quantitative confidence scores for archaeological potential</li>
                </ul>
                
                <h3> Scientific Innovation</h3>
                <p>This work represents a significant advancement in computational archaeology by combining:</p>
                <ul>
                    <li>Multi-modal remote sensing data fusion</li>
                    <li>Advanced AI models (GPT-4.1, o3-mini) for expert-level site evaluation</li>
                    <li>Biodiversity-informed archaeological site prediction</li>
                    <li>Fully automated and scalable analysis pipeline</li>
                </ul>
                
                <h3> Archaeological Significance</h3>
                <p>The Amazon basin holds countless secrets of pre-Columbian civilizations. This AI-powered approach opens new possibilities for:</p>
                <ul>
                    <li>Rapid large-scale archaeological survey</li>
                    <li>Preservation-focused site identification</li>
                    <li>Integration of indigenous knowledge with modern technology</li>
                    <li>Non-invasive preliminary site assessment</li>
                </ul>
                
                <h3> Future Directions</h3>
                <p>This methodology can be extended to:</p>
                <ul>
                    <li>Higher resolution satellite and LiDAR data integration</li>
                    <li>Integration with ground-penetrating radar data</li>
                    <li>Collaboration with local archaeologists for ground-truth validation</li>
                    <li>Expansion to other tropical archaeological regions worldwide</li>
                </ul>
                
                <div style="text-align: center; margin: 30px 0; padding: 20px; background: #e3f2fd; border-radius: 10px;">
                    <h4> OpenAI to Z Challenge Submission</h4>
                    <p><strong>Submission Date:</strong> {datetime.now().strftime('%B %d, %Y')}<br>
                    <strong>Analysis Timestamp:</strong> {RUN_TIMESTAMP}<br>
                    <strong>Challenge Track:</strong> Archaeological Site Discovery<br>
                    <strong>Geographic Focus:</strong> Amazon Basin, South America</p>
                </div>
            </div>
        </div>
        """
    
main_logger.info("Visualization and HTML Reporting System ready")

# ==============================================================================
# --- Phase 6: Main Orchestration and Expedition Management ---
# ==============================================================================

class SiteGenerator:
    """Generate candidate archaeological sites using AI."""
    
    def __init__(self, config: ConfigurationManager):
        self.config = config
        self.ai_manager = AIManager(config)
        main_logger.info("Site Generator initialized")
    
    def generate_candidate_sites(self, num_sites: int = 20, batch_size: int = 5) -> List[CandidateSite]:
        """Generate candidate archaeological sites using AI models."""
        main_logger.info(f"Generating {num_sites} candidate sites in batches of {batch_size}")
        
        all_sites = []
        num_batches = (num_sites + batch_size - 1) // batch_size
        
        for batch_num in range(num_batches):
            main_logger.info(f"Generating batch {batch_num + 1}/{num_batches}")
            
            prompt = f"""
Generate {min(batch_size, num_sites - len(all_sites))} promising candidate coordinates for undiscovered pre-Columbian archaeological sites in the Amazon basin.

Focus on areas with high archaeological potential based on:
- Proximity to major river systems (Amazon, Orinoco tributaries)
- Elevated but accessible terrain (100-600m elevation)
- Areas known for terra preta soils
- Historical accounts of indigenous settlements
- Remote but strategically located positions

Each site should have realistic coordinates within the Amazon basin and a plausible archaeological hypothesis.

Output as JSON with this exact structure:
{{
  "sites": [
    {{
      "coordinates": [longitude, latitude],
      "hypothesis": "Brief archaeological significance hypothesis"
    }}
  ]
}}

Ensure coordinates are within: Latitude -18 to 6N, Longitude -82 to -44W
"""
            
            try:
                response = self.ai_manager.make_structured_request(prompt, CandidateSites, 'analysis')
                if response and hasattr(response, 'sites'):
                    batch_sites = []
                    for site_data in response.sites:
                        # Validate coordinates
                        lon, lat = site_data.coordinates
                        if validate_coordinates(lat, lon):
                            site_id = create_site_id(lat, lon)
                            candidate = CandidateSite(
                                coordinates=(lon, lat),
                                hypothesis=site_data.hypothesis,
                                site_id=site_id,
                                confidence_score=50.0  # Initial neutral confidence
                            )
                            batch_sites.append(candidate)
                        else:
                            main_logger.warning(f"Invalid coordinates: {lat}, {lon}")
                    
                    all_sites.extend(batch_sites)
                    main_logger.info(f"Generated {len(batch_sites)} valid sites in batch {batch_num + 1}")
                else:
                    main_logger.error(f"Failed to generate sites for batch {batch_num + 1}")
                
                time.sleep(2)  # Rate limiting
                
            except Exception as e:
                main_logger.error(f"Error generating batch {batch_num + 1}: {e}")
        
        main_logger.info(f"Total sites generated: {len(all_sites)}")
        return all_sites
    
    def shortlist_sites(self, candidates: List[CandidateSite], num_shortlist: int = 10) -> List[CandidateSite]:
        """Shortlist the most promising sites using AI evaluation."""
        if len(candidates) <= num_shortlist:
            return candidates
        
        main_logger.info(f"Shortlisting {num_shortlist} sites from {len(candidates)} candidates")
        
        # Format candidates for AI review
        formatted_sites = []
        for i, site in enumerate(candidates):
            formatted_sites.append(f"Site {i+1}: {site.coordinates} - {site.hypothesis}")
        
        prompt = f"""
As expert Amazonian archaeologists, review these {len(candidates)} candidate sites and select the {num_shortlist} most scientifically promising for intensive analysis.

Consider:
- Archaeological significance and plausibility
- Geographic diversity across the Amazon basin
- Accessibility for potential future fieldwork
- Strategic importance for understanding Amazonian prehistory
- Likelihood of preservation

Candidates:
{chr(10).join(formatted_sites)}

Return the site numbers (1-{len(candidates)}) of your top {num_shortlist} choices as JSON:
{{
  "selected_sites": [1, 5, 12, ...]
}}
"""
        
        try:
            response = self.config.llm_client.chat.completions.create(
                model=LLM_MODEL_ANALYSIS,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                max_tokens=1000,
            )
            
            content = response.choices[0].message.content
            if content:
                import json
                result = json.loads(content)
                selected_indices = result.get('selected_sites', [])
                
                shortlisted = []
                for idx in selected_indices:
                    if 1 <= idx <= len(candidates):
                        shortlisted.append(candidates[idx - 1])  # Convert to 0-based indexing
                
                main_logger.info(f"Successfully shortlisted {len(shortlisted)} sites")
                return shortlisted
            
        except Exception as e:
            main_logger.error(f"Shortlisting failed: {e}")
        
        # Fallback to first N sites
        main_logger.warning("Using fallback shortlisting method")
        return candidates[:num_shortlist]

class AmazonArchaeologicalExpedition:
    """Main orchestration class for the OpenAI to Z Challenge expedition."""
    
    def __init__(self):
        main_logger.info("="*80)
        main_logger.info(" AMAZON ARCHAEOLOGICAL EXPEDITION - OpenAI to Z Challenge")
        main_logger.info("="*80)
        
        # Initialize all system components
        self.config = ConfigurationManager()
        self.site_generator = SiteGenerator(self.config)
        self.data_acquisition = UnifiedDataAcquisition(self.config)
        self.visualizer = VisualizationManager()
        self.html_reporter = HTMLReportGenerator()
        
        # Expedition state
        self.expedition_results = {
            'start_time': datetime.now(),
            'sites_analyzed': [],
            'recommended_site': None,
            'methodology': {},
            'maps_created': [],
            'research_summary': ''
        }
        
        main_logger.info("All expedition systems initialized successfully")
    
    def run_expedition(self, num_candidate_sites: int = 20, num_shortlisted: int = 10, 
                      max_iterations: int = 3) -> str:
        """Run the complete archaeological expedition."""
        try:
            main_logger.info(f" Starting expedition: {num_candidate_sites} candidates  {num_shortlisted} shortlisted")
            
            # Phase 1: Generate candidate sites
            main_logger.info(" Phase 1: Generating candidate archaeological sites")
            candidate_sites = self.site_generator.generate_candidate_sites(num_candidate_sites)
            
            if not candidate_sites:
                main_logger.error("No candidate sites generated. Expedition aborted.")
                return ""
            
            # Phase 2: Shortlist promising sites
            main_logger.info(" Phase 2: Shortlisting most promising sites")
            shortlisted_sites = self.site_generator.shortlist_sites(candidate_sites, num_shortlisted)
            
            # Phase 3: Comprehensive analysis of shortlisted sites
            main_logger.info(" Phase 3: Comprehensive analysis of shortlisted sites")
            analysis_results = []
            
            for i, site in enumerate(shortlisted_sites):
                main_logger.info(f"Analyzing site {i+1}/{len(shortlisted_sites)}: {site.site_id}")
                
                try:
                    # Run the agentic workflow for this site
                    site_results = self._analyze_site_with_workflow(site, max_iterations)
                    analysis_results.append(site_results)
                    
                    # Create visualization maps
                    if site_results.get('data_collection_results'):
                        map_path = self.visualizer.create_site_overview_map(
                            site, 
                            site_results['data_collection_results'],
                            site_results.get('analysis_results', {})
                        )
                        if map_path:
                            self.expedition_results['maps_created'].append(map_path)
                    
                except Exception as e:
                    main_logger.error(f"Analysis failed for {site.site_id}: {e}")
                    # Create minimal result structure
                    analysis_results.append({
                        'site': site.model_dump(),
                        'error': str(e),
                        'confidence_score': 0,
                        'iteration': 0
                    })
            
            # Phase 4: Select recommended site
            main_logger.info(" Phase 4: Selecting recommended discovery")
            recommended_site = self._select_recommended_site(analysis_results)
            
            # Phase 5: Generate comprehensive reports
            main_logger.info(" Phase 5: Generating comprehensive reports")
            
            # Create interactive map
            sites_for_map = [CandidateSite(**result['site']) for result in analysis_results if 'site' in result]
            interactive_map = self.visualizer.create_interactive_map(sites_for_map, analysis_results)
            
            # Update expedition results
            self.expedition_results.update({
                'sites_analyzed': analysis_results,
                'recommended_site': recommended_site,
                'interactive_map': interactive_map,
                'methodology': self._generate_methodology_summary(),
                'end_time': datetime.now()
            })
            
            # Generate final HTML report
            html_report = self.html_reporter.generate_comprehensive_report(self.expedition_results)
            
            # Generate summary
            self._generate_expedition_summary()
            
            main_logger.info("="*80)
            main_logger.info(" EXPEDITION COMPLETED SUCCESSFULLY!")
            main_logger.info(f" HTML Report: {html_report}")
            main_logger.info(f" Interactive Map: {interactive_map}")
            main_logger.info(f" All outputs: {BASE_DIR}")
            main_logger.info("="*80)
            
            return html_report
            
        except Exception as e:
            main_logger.error(f"Expedition failed: {e}")
            import traceback
            main_logger.error(traceback.format_exc())
            return ""
    
    def _analyze_site_with_workflow(self, site: CandidateSite, max_iterations: int) -> Dict[str, Any]:
        """Analyze a single site using the agentic workflow."""
        from langgraph.graph import StateGraph, END
        
        # Build the workflow graph
        builder = StateGraph(ArchaeologicalState)
        
        # Add nodes
        builder.add_node("data_collection", data_collection_node)
        builder.add_node("analysis", analysis_node)
        builder.add_node("web_search", web_search_node)
        builder.add_node("vector_search", vector_search_node)
        builder.add_node("ai_evaluation", ai_evaluation_node)
        
        # Define workflow edges
        builder.set_entry_point("data_collection")
        builder.add_edge("data_collection", "analysis")
        builder.add_edge("analysis", "web_search")
        builder.add_edge("web_search", "vector_search")
        builder.add_edge("vector_search", "ai_evaluation")
        
        # Conditional edges for iteration
        builder.add_conditional_edges(
            "ai_evaluation",
            should_continue_research,
            {
                "continue": "web_search",  # Continue iterative research
                "finalize": END            # End workflow
            }
        )
        
        # Compile workflow
        workflow = builder.compile()
        
        # Initial state
        initial_state = {
            "iteration": 0,
            "max_iterations": max_iterations,
            "site": site,
            "hypothesis": site.hypothesis,
            "confidence_score": site.confidence_score or 50.0,
            "evidence_summary": [],
            "ai_evaluations": [],
            "data_collection_results": {},
            "analysis_results": {},
            "web_search_results": {},
            "vector_search_results": [],
            "critique": "",
            "final_recommendation": None
        }
        
        # Run workflow
        final_state = None
        try:
            for event in workflow.stream(initial_state, {"recursion_limit": 50}):
                final_state = event
            
            # Extract final state values
            if final_state:
                result_state = list(final_state.values())[0]
                # Ensure site is stored as dict for consistency
                if 'site' in result_state and hasattr(result_state['site'], 'model_dump'):
                    result_state['site'] = result_state['site'].model_dump()
                return result_state
            else:
                # Ensure site is stored as dict for consistency
                initial_state['site'] = initial_state['site'].model_dump()
                return initial_state
                
        except Exception as e:
            main_logger.error(f"Workflow execution failed for {site.site_id}: {e}")
            return initial_state
    
    def _select_recommended_site(self, analysis_results: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Select the most promising site for recommendation."""
        valid_results = [r for r in analysis_results if r.get('confidence_score', 0) > 0]
        
        if not valid_results:
            main_logger.warning("No sites with valid confidence scores found")
            return None
        
        # Sort by confidence score
        sorted_results = sorted(valid_results, 
                              key=lambda x: x.get('confidence_score', 0), 
                              reverse=True)
        
        recommended = sorted_results[0]
        confidence = recommended.get('confidence_score', 0)
        
        main_logger.info(f"Recommended site: {recommended.get('site', {}).get('site_id', 'Unknown')} "
                        f"(Confidence: {confidence:.1f}%)")
        
        return recommended
    
    def _generate_methodology_summary(self) -> Dict[str, Any]:
        """Generate methodology summary for the report."""
        return {
            'ai_models_used': ['GPT-4.1', 'o3-mini', 'LLM'],
            'data_sources': [
                'Sentinel-2 L2A (10m optical)',
                'Landsat 8/9 (30m optical)', 
                'Sentinel-1 SAR (10m)',
                'SRTM DEM (30m)',
                'NASA DEM (30m)',
                'GBIF biodiversity data',
                'OpenTopography LiDAR'
            ],
            'analysis_techniques': [
                'Multi-scale anomaly detection',
                'Geometric feature analysis',
                'Vegetation index calculation',
                'SAR backscatter analysis',
                'Biodiversity-informed site prediction',
                'Agentic iterative refinement'
            ],
            'evaluation_criteria': [
                'Geometric regularity',
                'Terrain modifications',
                'Vegetation anomalies', 
                'Archaeological context',
                'AI confidence assessment'
            ]
        }
    
    def _generate_expedition_summary(self):
        """Generate and log expedition summary."""
        results = self.expedition_results
        
        total_sites = len(results['sites_analyzed'])
        high_confidence = len([s for s in results['sites_analyzed'] 
                             if s.get('confidence_score', 0) > 75])
        medium_confidence = len([s for s in results['sites_analyzed'] 
                               if 50 <= s.get('confidence_score', 0) <= 75])
        
        duration = results['end_time'] - results['start_time']
        
        summary = f"""
        
 AMAZON ARCHAEOLOGICAL EXPEDITION SUMMARY


 ANALYSIS STATISTICS:
    Total Sites Analyzed: {total_sites}
    High Confidence Sites (>75%): {high_confidence}
    Medium Confidence Sites (50-75%): {medium_confidence}
    Maps Generated: {len(results['maps_created'])}
    Duration: {duration}

 RECOMMENDED DISCOVERY:
               {f" Site ID: {results['recommended_site'].get('site', {}).get('site_id', 'Unknown')}" if results['recommended_site'] else " No high-confidence sites identified"}
               {f" Confidence: {results['recommended_site']['confidence_score']:.1f}%" if results['recommended_site'] else ""}
            {f" Location: {results['recommended_site'].get('site', {}).get('coordinates', 'Unknown')}" if results['recommended_site'] else ""}

 DATA SOURCES UTILIZED:
    Sentinel-2 L2A optical imagery (10m resolution)
    Landsat 8/9 multispectral (30m resolution)
    Sentinel-1 SAR data (10m resolution)
    Multiple DEM sources (SRTM, NASA, Copernicus)
    GBIF biodiversity occurrence data
    OpenTopography LiDAR data

 AI MODELS DEPLOYED:
    GPT-4.1 for expert archaeological analysis
    o3-mini for rapid hypothesis generation
    LLM for specialized remote sensing evaluation

 OUTPUTS GENERATED:
    Comprehensive HTML report
    Interactive discovery map
    Individual site analysis maps
    Quantitative confidence scores
    Reproducible methodology documentation


        """
        
        main_logger.info(summary)
        
        # Save summary to file
        summary_file = OUTPUT_DIR / f"expedition_summary_{RUN_TIMESTAMP}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        main_logger.info(f"Expedition summary saved to: {summary_file}")

# ==============================================================================
# --- Main Execution ---
# ==============================================================================

def main():
    """Main execution function for the OpenAI to Z Challenge."""
    try:
        # Create and run the expedition
        expedition = AmazonArchaeologicalExpedition()
        
        # Run with parameters suitable for the challenge
        html_report = expedition.run_expedition(
            num_candidate_sites=15,  # Generate 15 candidate sites
            num_shortlisted=8,       # Shortlist 8 for detailed analysis
            max_iterations=3         # 3 iterations of refinement per site
        )
        
        if html_report:
            print(f"\n SUCCESS: OpenAI to Z Challenge submission ready!")
            print(f" Main Report: {html_report}")
            print(f" All Files: {BASE_DIR}")
            print(f"\n Archaeological discovery expedition completed successfully!")
        else:
            print(" Expedition failed. Check logs for details.")
    
    except Exception as e:
        print(f" Critical error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

main_logger.info("Amazon Archaeological Discovery System - Ready for OpenAI to Z Challenge")